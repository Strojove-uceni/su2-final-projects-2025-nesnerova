{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKmFQUkbbietZSHtynlZkx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/su2-final-projects-2025-nesnerova/blob/main/SU2_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Z√°vƒõreƒçn√Ω projekt k p≈ôedmƒõtu SU2\n",
        "# Detekce a sledov√°n√≠ klatrinem pota≈æen√Ωch jamek pomoc√≠ metod strojov√©ho uƒçen√≠\n",
        "\n",
        "Autor: Sabina Ne≈°nƒõrov√°\n"
      ],
      "metadata": {
        "id": "xfI3qv28gtyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Abstrakt\n",
        "\n",
        "Tento projekt se zab√Ωv√° aplikac√≠ metod strojov√©ho uƒçen√≠ p≈ôi anal√Ωze mikroskopick√Ωch dat TIRF-SIM s c√≠lem detekovat a sledovat klatrinem pota≈æen√© jamky (CCP) bƒõhem endocyt√≥zy. V√Ωchoz√≠ baseline metody (jednoduch√Ω morfologick√Ω detektor, CNN a Nearest-Neighbour tracker) vykazovaly n√≠zkou p≈ôesnost, a proto byly implementov√°ny a experiment√°lnƒõ porovn√°ny pokroƒçilej≈°√≠ p≈ô√≠stupy.\n",
        "\n",
        "V projektu byly porovn√°v√°ny r≈Øzn√© architektury neuronov√Ωch s√≠t√≠ (U-Net, Res-U-Net), r≈Øzn√© postupy filtrov√°n√≠ v√Ωstupu a nƒõkolik metod trackingu ( Kalman≈Øv filtr, Nearest-Neighbour p≈ô√≠stup a knihovna Norfair). Souƒç√°st√≠ pr√°ce bylo rovnƒõ≈æ ladƒõn√≠ parametr≈Ø (threshold, max_dist, sigma).\n",
        "\n",
        "P≈ôesto≈æe se nepoda≈ôilo p≈ôekonat referenƒçn√≠ metodu SOTA, dosa≈æen√© v√Ωsledky v√Ωraznƒõ p≈ôekonaly p≈Øvodn√≠ baseline a p≈ôibl√≠≈æily se kvalitƒõ SOTA v metrik√°ch HOTA, AssA a DetA. Projekt tak demonstruje, jak lze kombinac√≠ pokroƒçilej≈°√≠ch segmentaƒçn√≠ch model≈Ø a sofistikovanƒõj≈°√≠ho trackingu v√Ωznamnƒõ zlep≈°it anal√Ωzu dynamiky CCP.\n"
      ],
      "metadata": {
        "id": "wq5oRulQgvUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metodika\n",
        "P≈ôi bunƒõƒçn√© endocyt√≥ze doch√°z√≠ k procesu, kdy bu≈àka p≈ôij√≠m√° molekuly a dal≈°√≠ men≈°√≠ bu≈àky. Klatrinem pota≈æen√© jamky tzv. CCP se vytv√°≈ôej√≠ na povrchu bu≈àky, rostou a n√°slednƒõ se oddƒõl√≠ a t√≠m vytvo≈ô√≠ tzv. vezikuly, kter√© pak p≈ôen√°≈°√≠ pot≈ôebn√© ≈æiviny do bu≈àky.\n",
        "\n",
        "Studium tohoto procesu p≈ôin√°≈°√≠ vƒõdc≈Øm d≈Øle≈æit√© informace o bunƒõƒçn√Ωch p≈ôemƒõn√°ch, p≈ô√≠jmu ≈æivin a podobnƒõ. Probl√©mem v≈°ak je, jak detekovat a sledovat mikroskopick√° data. Tuto ot√°zku ≈ôe≈°√≠ strojov√© uƒçen√≠ a jeho metody aplikovan√© na mikroskopick√° data, kter√° jsou upravena tzv. TIRF-SIM metodou, co≈æ je metoda kombinuj√≠c√≠ osvƒõtlen√≠ pouze tenk√© vrstvy bu≈àky, d√≠ky ƒçemu≈æ se m≈Ø≈æeme zamƒõ≈ôit pouze na povrch bu≈àky, a rozli≈°en√≠ nad r√°mec klasick√Ωch svƒõteln√Ωch mikroskoup≈Ø. D√≠ky t√©to metodƒõ jsme schopni jasnƒõ pozorovat CCP jamky a jejich p≈ôemƒõny.\n",
        "\n",
        "Prim√°rn√≠m c√≠lem projektu je co nejv√≠ce se p≈ôibl√≠≈æit v√Ωkonu referenƒçn√≠ metody SOTA, p≈ô√≠padnƒõ dos√°hnout je≈°tƒõ lep≈°√≠ch v√Ωsledk≈Ø. Metoda SOTA je metoda dosahuj√≠c√≠ nejlep≈°√≠ch v√Ωsledk≈Ø v √∫loze mikroskopick√Ωch dat.\n",
        "\n",
        "V t√©to √∫loze byly prvn√≠ vstupn√≠ metody Konvoluƒçn√≠ neuronov√° s√≠≈• a Jednoduch√Ω morfologick√Ω detektor spolu se SOTA vyhodnoceny n√°sledovnƒõ:\n",
        "\n",
        "| Metrika | SOTA | P≈Øvodn√≠ CNN |P≈Øvodn√≠ Morphological|\n",
        "|---------|------|-------------|----------------------|\n",
        "| HOTA    | 0.74 | 0.31        |         0.23         |\n",
        "| AssA    | 0.67 | 0.27        |             0.23     |\n",
        "| DetA    | 0.81 | 0.36        |                 0.24 |\n",
        "\n",
        "\n",
        "Na≈°√≠m c√≠lem je tedy zv√Ω≈°it obƒõ metriky AssA i DetA, ƒç√≠m≈æ se zv√Ω≈°√≠ i v√Ωsledn√° HOTA. Jak lze vidƒõt z tabulky u p≈Øvodn√≠ CNN byl obecnƒõ nejvƒõt≈°√≠ probl√©m s AssA metrikou, kter√° se vztahuje k trackingu.\n",
        "\n",
        "Budou porovn√°ny dvƒõ metody detektoru a dvƒõ metody trackingu, p≈ôiƒçem≈æ v z√°vƒõru bude uvedena nejlep≈°√≠ kombinace spolu s parametry d√°vaj√≠c√≠ nejbli≈æ≈°√≠ ƒçislo HOTA k SOTA metodƒõ.\n",
        "\n",
        "##U-Net\n",
        "U-Net je konvoluƒçn√≠ neuronov√° s√≠≈• navr≈æen√° pro segmentaci obrazu na √∫rovni pixel≈Ø. Jej√≠ architektura se skl√°d√° z kod√©ru, kter√Ω extrahuje rysy, a dekod√©ru, kter√Ω je opƒõt p≈ôev√°d√≠ na prostorov√© mapy.\n",
        "V tomto projektu U-Net odhaduje pravdƒõpodobnostn√≠ mapu v√Ωskytu CCP, z n√≠≈æ se pot√© z√≠skaj√≠ lok√°ln√≠ maxima jako detekce.\n",
        "\n",
        "Byla vybr√°na z tƒõchto d≈Øvod≈Ø:\n",
        "- detekuje mal√© objekty\n",
        "- Zlep≈°uje AssA (m√©nƒõ ƒçast√© p≈ôeskakov√°n√≠ mezi ID)\n",
        "\n",
        "Model byl natr√©nov√°n v souboru [training_unet](https://raw.githubusercontent.com/Strojove-uceni/su2-final-projects-2025-nesnerova/refs/heads/main/training_unet.ipynb) a zde jsou jen sta≈æeny v√Ωsledky k dal≈°√≠ interpretaci\n",
        "\n",
        "\n",
        "##ResUNet\n",
        "Tato metoda je vylep≈°enou verz√≠ U-Net d√≠ky residu√°ln√≠m blok≈Øm p≈ôevzat√Ωm z ResNet.\n",
        "\n",
        "Byla vybr√°na z tƒõchto d≈Øvod≈Ø\n",
        "- stabiln√≠ tr√©nink\n",
        "- rychlej≈°√≠ konvergence\n",
        "- schopnost uƒçit se jemnƒõj≈°√≠m detail≈Øm\n",
        "\n",
        "Model byl natr√©nov√°n v souboru [training_resunet](https://raw.githubusercontent.com/Strojove-uceni/su2-final-projects-2025-nesnerova/refs/heads/main/training_resunet.ipynb) a zde jsou jen sta≈æeny v√Ωsledky k dal≈°√≠ interpretaci\n",
        "\n",
        "##Nearest-Neighbor\n",
        "Tento p≈ô√≠stup byl ponech√°z z p≈Øvodn√≠ho baseline souboru pro porovn√°n√≠ s metodami zm√≠nƒõn√Ωmi n√≠≈æe. Je zalo≈æen na p≈ôi≈ôazen√≠ objektu z dal≈°√≠ho sn√≠mku k objektu z p≈ôedchoz√≠ho sn√≠mku podle vzd√°lenosti.\n",
        "\n",
        "##Kalman≈Øv filtr\n",
        "Tato metoda je urƒçen√° pro odhady stav≈Ø dynamick√Ωch syst√©m≈Ø, kter√© jsou zat√≠≈æeny ≈°umem, co≈æ je pr√°vƒõ p≈ô√≠pad bunƒõƒçn√© endocyt√≥zy. Funguje na z√°kladƒõ dvou krok≈Ø - predikce polohy na z√°kladƒõ p≈ôedchoz√≠ho stavu a n√°sledn√° korekce podle nov√© detekce.\n",
        "\n",
        "Byla vybr√°na z tƒõchto d≈Øvod≈Ø:\n",
        "- Odhaduje za≈°umƒõn√© syst√©my\n",
        "- p≈ôesn√° detekce poloh a spojen√≠ v trajektorie\n",
        "- zlep≈°uje DetA\n",
        "\n",
        "##Hungarian algoritmus\n",
        "Optimalizaƒçn√≠ metoda pro p≈ôi≈ôazov√°n√≠ objekt≈Ø k sobƒõ. Zde se p≈ôi≈ôazuj√≠ k sobƒõ predikce z Kalmanova filtru a nov√© detekce sn√≠mku\n",
        "\n",
        "##Knihovna Norfair\n",
        "Norfair je modern√≠ knihovna pro multi-object tracking, kter√° pou≈æ√≠v√° jednoduch√© aktualizaƒçn√≠ pravidlo a r≈Øzn√© metriky vzd√°lenosti. Byla pou≈æita Manhattanova vzd√°lenost\n",
        "V projektu byla testov√°na jako alternativa ke Kalmanovu filtru."
      ],
      "metadata": {
        "id": "B2XcYaebg2US"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaOIAVg1gpbp"
      },
      "outputs": [],
      "source": [
        "#@title Instalace pot≈ôebn√Ωch bal√≠ƒçk≈Ø\n",
        "\n",
        "!pip install filterpy\n",
        "!pip install norfair\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Propojen√≠ s GitHub\n",
        "!rm -rf project\n",
        "!git clone https://github.com/Strojove-uceni/su2-final-projects-2025-nesnerova.git project\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/project\")"
      ],
      "metadata": {
        "id": "3rHudNjSg6Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import knihoven\n",
        "\n",
        "from src.models import UNet, ResUNet\n",
        "import gdown\n",
        "import os, requests, zipfile\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import subprocess\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['animation.embed_limit'] = 150\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import ScalarMappable\n",
        "from matplotlib.colors import Normalize\n",
        "import matplotlib.collections as mc\n",
        "import pandas as pd\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from torch.utils.data import IterableDataset\n",
        "from typing import Tuple, Union\n",
        "from scipy.ndimage import gaussian_filter, maximum_filter\n",
        "import ipywidgets as widgets\n",
        "import math\n",
        "from scipy import spatial, optimize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as torch_data\n",
        "import time\n",
        "import threading\n",
        "from norfair import Detection, Tracker\n",
        "import random\n",
        "from filterpy.kalman import KalmanFilter\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "mQ2CuLnUg_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sta≈æen√≠ TIRF-SIM dat\n",
        "\n",
        "def download_and_unzip(url, extract_to, chain_path):\n",
        "    \"\"\"Download and unzip using a verified SSL certificate.\"\"\"\n",
        "    if os.path.exists(extract_to):\n",
        "        print(f\"The directory '{extract_to}' already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    local_zip = os.path.basename(url)\n",
        "    print(f\"üì• Downloading {local_zip} (approx 1min)...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, verify=chain_path, timeout=20)\n",
        "        response.raise_for_status()\n",
        "        with open(local_zip, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"‚úÖ {local_zip} downloaded successfully ({os.path.getsize(local_zip)} bytes)\")\n",
        "\n",
        "        print(f\"üìÇ Extracting to '{extract_to}/'...\")\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        with zipfile.ZipFile(local_zip, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        os.remove(local_zip)\n",
        "        print(f\"‚úÖ Extraction completed to '{extract_to}'\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Download error: {e}\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"‚ùå The downloaded file is not a valid ZIP archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Unexpected error: {e}\")\n",
        "\n",
        "chain_path = \"/content/chain-harica-cross.pem\"\n",
        "print(\"üîê Downloading SSL certificate chain...\")\n",
        "cert_url = \"https://pki.cesnet.cz/_media/certs/chain-harica-rsa-ov-crosssigned-root.pem\"\n",
        "r = requests.get(cert_url, timeout=10, stream=True)\n",
        "r.raise_for_status()\n",
        "with open(chain_path, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "print(\"‚úÖ Certificate chain downloaded.\\n\")\n",
        "zip_url = \"https://su2.utia.cas.cz/files/labs/final2025/val_and_sota.zip\"\n",
        "extract_directory = \"/content/val_data\"\n",
        "download_and_unzip(zip_url, extract_directory, chain_path)\n",
        "\n",
        "print(\"\\nüéØ All done!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OITUVtXHg_GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Funkce z baseline\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Ensure deterministic behavior in cuDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "def open_tiff_file(name: str) -> np.ndarray:\n",
        "    img = Image.open(name)\n",
        "    frames = []\n",
        "    for i in range(img.n_frames):\n",
        "        img.seek(i)\n",
        "        frames.append(np.array(img))\n",
        "\n",
        "    return np.array(frames).squeeze()\n",
        "\n",
        "def loading_html(message: str) -> str:\n",
        "    return f\"\"\"\n",
        "<div id=\"loading-msg\">\n",
        "  <br /><br />\n",
        "  <b><span style='display:inline-block;animation:flipPause 2s ease infinite;'>‚è≥</span>\n",
        "  {message}</b>\n",
        "</div>\n",
        "<style>\n",
        "@keyframes flipPause {{\n",
        "  0% {{transform:rotate(0deg);}}\n",
        "  40%{{transform:rotate(180deg);}}\n",
        "  50%{{transform:rotate(180deg);}}\n",
        "  90%{{transform:rotate(360deg);}}\n",
        "  100%{{transform:rotate(360deg);}}\n",
        "}}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "html_code_reconstruction = loading_html(\"Showing input validation data, please wait...\")\n",
        "\n",
        "def replace_loading_js(message: str, delay_ms: int = 0) -> str:\n",
        "    return f\"\"\"\n",
        "<script>\n",
        "  setTimeout(function(){{\n",
        "    var loadingDiv = document.getElementById(\"loading-msg\");\n",
        "    if (loadingDiv) {{\n",
        "      loadingDiv.innerHTML = '<br /><b>{message}</b>';\n",
        "    }}\n",
        "  }}, {delay_ms});\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "replace_loading_js_default = replace_loading_js(\"Only the first 50 frames are displayed.\")\n",
        "replace_loading_js_empty = replace_loading_js(\"\")\n",
        "\n",
        "\n",
        "class OTF:\n",
        "    \"\"\"\n",
        "    The Optical Transfer Function of the optical system.\n",
        "    Generates a 2-D image from an exponential approximation of the ideal OTF.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, na: float, wavelength: float, pixel_size: float, image_size: int, curvature: float):\n",
        "        \"\"\"\n",
        "        :param na: Numerical aperture of the optical system.\n",
        "        :param wavelength: Wavelength of the emitted light in nanometers.\n",
        "        :param pixel_size: Physical size of the image pixel in micrometers.\n",
        "        :param image_size: Width and height of the image.\n",
        "        :param curvature: Bend of the model OTF function in [0, 1] range where 1 is a perfect OTF.\n",
        "        :param samples: Number of values to sample.\n",
        "        \"\"\"\n",
        "        cutoff_frequency = 1000 * 2 * na / wavelength  # in micrometer^-1 (for incoherent imaging)\n",
        "        self.image_cutoff = cutoff_frequency * pixel_size * image_size\n",
        "        self.image_size = image_size\n",
        "        self.curvature = curvature\n",
        "\n",
        "    def __call__(self, size: int = None, x_shift: float = 0, y_shift: float = 0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate a 2-D image representation.\n",
        "        :param size: Width and height of the generated image.\n",
        "        :param x_shift: Sub-pixel shift along the x-axis of the OTF center.\n",
        "        :param y_shift:Sub-pixel shift along the y-axis of the OTF center.\n",
        "        :return: 2-D representation of the OTF\n",
        "        \"\"\"\n",
        "        if size is None:\n",
        "            size = self.image_size\n",
        "\n",
        "        x, y = np.meshgrid(np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]),\n",
        "                           np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]))\n",
        "        distance_to_origin = np.hypot(x + x_shift, y + y_shift)\n",
        "\n",
        "        return self.value(np.minimum(distance_to_origin / self.image_cutoff, 1))\n",
        "\n",
        "    def double(self, x_shift: float = 0, y_shift: float = 0) -> np.ndarray:\n",
        "        return self(self.image_size * 2, x_shift, y_shift)\n",
        "\n",
        "    def value(self, x):\n",
        "        return (2 / np.pi) * (np.arccos(x) - x * np.sqrt(1 - x * x)) * self.curvature ** x\n",
        "\n",
        "def apodization_filter(dist_ratio: float, bend: float, size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate an apodization filter that can be directly multiplied with the summed fft result image.\n",
        "\n",
        "    :param dist_ratio: A coefficient that transform the point distance in pixels to a value where [0, 1] range\n",
        "        corresponds to the extended OTF support.\n",
        "    :param bend: A coefficient in the [0, 1] range defining how much medium frequencies should be augmented.\n",
        "    :param size: Size of the apodization filter.\n",
        "    :return: np.ndarray of size (size, size) of the apodization filter.\n",
        "    \"\"\"\n",
        "    x, y = np.meshgrid(np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]),\n",
        "                       np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]))\n",
        "\n",
        "    distance = np.hypot(x, y) * dist_ratio\n",
        "    mask = np.bitwise_and(0 <= distance, distance < 1)\n",
        "\n",
        "    apo = np.power(\n",
        "        (2 / np.pi) * (np.arccos(distance, where=mask) - distance * np.sqrt(1 - distance * distance, where=mask)),\n",
        "        bend,\n",
        "        where=mask\n",
        "    )\n",
        "\n",
        "    return np.where(mask, apo, 0)\n",
        "\n",
        "\n",
        "def wiener_filter(shifts, otf: OTF, w: float, size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate a wiener filter denominator so that it can be directly multiplied with the summed fft result image.\n",
        "\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :param otf: OTF object.\n",
        "    :param w: Wiener parameter.\n",
        "    :param size: Size of the wiener filter.\n",
        "    :return: np.ndarray of size (size, size) of the Wiener filter denominator.\n",
        "    \"\"\"\n",
        "    otf0 = np.abs(otf(size)) ** 2\n",
        "    wiener = 3 * otf0\n",
        "\n",
        "    for i in range(3):\n",
        "        otf1 = np.abs(otf(size, shifts[i][1], shifts[i][0])) ** 2\n",
        "        otf2 = np.abs(otf(size, -shifts[i][1], -shifts[i][0])) ** 2\n",
        "\n",
        "        wiener += otf1 + otf2\n",
        "\n",
        "    return 1 / (wiener + w * w)\n",
        "\n",
        "def map_otf_support(components: np.ndarray, shifts, otf: OTF) -> None:\n",
        "    \"\"\"\n",
        "    Multiply conjugated OTF to corresponding parts of it's support in the separated and shifted components. Values\n",
        "    outside the OTF support are set to zero. This function alters the original images.\n",
        "\n",
        "    :param components: np.ndarray of shape (9, size, size) of padded separated and shifted components in frequency space.\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :param otf: OTF object\n",
        "    \"\"\"\n",
        "    size = components.shape[-1]\n",
        "\n",
        "    components[::3] *= np.conjugate(otf(size))\n",
        "    for i in range(3):\n",
        "        components[i * 3 + 1] *= np.conjugate(otf(size, shifts[i][1], shifts[i][0]))\n",
        "        components[i * 3 + 2] *= np.conjugate(otf(size, -shifts[i][1], -shifts[i][0]))\n",
        "\n",
        "\n",
        "def fourier_shift(fft_image: np.ndarray, x_shift: Union[float, np.ndarray], y_shift: Union[float, np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Use the fourier shift theorem to perform a sub-pixel shift of a frequency image in the spatial domain.\n",
        "\n",
        "    It is implemented in a way that allows the x_shift and y_shift parameters to be a np.ndarray of arbitrary shape with\n",
        "    the last two axes being np.newaxis or None. The returned array shape is then x/y_shift.shape + (height, width)\n",
        "\n",
        "    :param fft_image: np.ndarray of shape (height, width).\n",
        "    :param x_shift: Sub-pixel shift along the x-axis.\n",
        "    :param y_shift: Sub-pixel shift along the y-axis.\n",
        "    :return: np.ndarray of shape (height, width) of the shifted frequency image.\n",
        "    \"\"\"\n",
        "    height, width = fft_image.shape[-2:]\n",
        "\n",
        "    spatial_image = np.fft.ifft2(fft_image)\n",
        "\n",
        "    x_indices = np.arange(-width // 2, width // 2, dtype=int)\n",
        "    y_indices = np.arange(-height // 2, height // 2, dtype=int)\n",
        "\n",
        "    x = np.exp(-1j * 2 * np.pi * x_shift * x_indices[None, :] / width)\n",
        "    y = np.exp(-1j * 2 * np.pi * y_shift * y_indices[:, None] / height)\n",
        "\n",
        "    shifted_spatial_image = spatial_image * (x * y)\n",
        "\n",
        "    return np.fft.fft2(shifted_spatial_image)\n",
        "\n",
        "\n",
        "def shift_components(components: np.ndarray, shifts) -> None:\n",
        "    \"\"\"\n",
        "    Use the fourier shift theorem to perform a sub-pixel shift of frequency components in the spatial domain. This\n",
        "    function alters the original images.\n",
        "\n",
        "    :param components: np.ndarray of shape (9, height, width) of padded separated components in frequency space.\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :return: np.ndarray of shape (9, height, width) of shifted components.\n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        components[i * 3 + 1, ...] = fourier_shift(components[i * 3 + 1, ...], shifts[i][1], shifts[i][0])\n",
        "        components[i * 3 + 2, ...] = fourier_shift(components[i * 3 + 2, ...], -shifts[i][1], -shifts[i][0])\n",
        "\n",
        "\n",
        "def pad_components(components: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pad an image to twice its size from the center, keeping the corners intact.\n",
        "    This makes it possible to pad images that are NOT centered using np.fft.fftshift.\n",
        "\n",
        "    :param components: np.ndarray of shape (..., size, size) of separated components in frequency space.\n",
        "    :return: np.ndarray of shape (..., size, size) padded with zeros from the center.\n",
        "    \"\"\"\n",
        "    size = components.shape[-1]\n",
        "    padded_components = np.zeros(components.shape[:-2] + (size * 2, size * 2), dtype=np.complex128)\n",
        "\n",
        "    x, y = np.meshgrid(\n",
        "        np.hstack([np.arange(size // 2), np.arange(size * 3 // 2, size * 2)]),\n",
        "        np.hstack([np.arange(size // 2), np.arange(size * 3 // 2, size * 2)])\n",
        "    )\n",
        "\n",
        "    padded_components[..., y, x] = components\n",
        "\n",
        "    return padded_components\n",
        "\n",
        "def _component_separation_matrix(phase_offset: float = 0, modulation: float = 1) -> np.ndarray:\n",
        "    phases = np.array([0, 2 * np.pi / 3, 4 * np.pi / 3]) + phase_offset\n",
        "\n",
        "    M = np.ones((3, 3), dtype=np.complex128)\n",
        "\n",
        "\n",
        "    M[:, 1] = 0.5 * modulation * (np.cos(phases) + 1j * np.sin(phases))\n",
        "    M[:, 2] = 0.5 * modulation * (np.cos(-phases) + 1j * np.sin(-phases))\n",
        "\n",
        "    return np.linalg.inv(M)\n",
        "\n",
        "def separate_components(fft_images: np.ndarray, phase_offsets=np.zeros(3), phase_modulations=np.ones(3)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the spectral separation of components.\n",
        "\n",
        "    :param fft_images: np.ndarray of shape (9, height, width) of images in frequency space.\n",
        "    :param phase_offsets: A container of length 3 of phase offsets for each of the 3 orientations, zeros by default.\n",
        "    :param phase_modulations: A container of length 3 of phase modulations for each of the 3 orientations,\n",
        "        ones by default.\n",
        "    :return: np.ndarray of shape (9, height, width) of separated components.\n",
        "    \"\"\"\n",
        "    separation_matrices = np.array([_component_separation_matrix(phase_offsets[i], phase_modulations[i])\n",
        "                                    for i in range(3)])\n",
        "\n",
        "    return np.einsum('aij,ajkl->aikl',separation_matrices,\n",
        "                     fft_images.reshape(3, 3, *fft_images.shape[1:])).reshape(9, *fft_images.shape[1:])\n",
        "\n",
        "\n",
        "def run_reconstruction(fft_images: np.ndarray, otf: OTF, shifts, phase_offsets, modulations, config: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Perform a SIM reconstruction using the provided parameters.\n",
        "\n",
        "    :param fft_images: np.ndarray of shape (9, height, width) of images in frequency space.\n",
        "    :param otf: OTF object\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :param phase_offsets: A container of length 3 of phase offsets for each of the 3 orientations.\n",
        "    :param modulations: A container of length 3 of phase modulations for each of the 3 orientations.\n",
        "    :param config: A dictionary of SIM reconstruction parameters. It must contain the following keys: [\"wavelength\",\n",
        "        \"na\", \"px_size\", \"apo_cutoff\", \"wiener_parameter\", \"apo_bend\"]\n",
        "    :return: fft_result, spatial_result\n",
        "    \"\"\"\n",
        "    size = fft_images.shape[-1]\n",
        "    cutoff = 1000 * 2 * config[\"na\"] / config[\"wavelength\"]\n",
        "    apo_dist_ratio = 1 / (config[\"px_size\"] * config[\"apo_cutoff\"] * cutoff * size)\n",
        "\n",
        "    components = separate_components(fft_images, phase_offsets, modulations)\n",
        "    components = pad_components(components)\n",
        "\n",
        "    shift_components(components, shifts)\n",
        "    map_otf_support(components, shifts, otf)\n",
        "\n",
        "    wiener = wiener_filter(shifts, otf, config[\"wiener_parameter\"], size * 2)\n",
        "    apodization = apodization_filter(apo_dist_ratio, config[\"apo_bend\"], size * 2)\n",
        "\n",
        "    fft_result = np.sum(components, 0) * wiener * apodization\n",
        "    spatial_result = np.real(np.fft.ifft2(fft_result))\n",
        "\n",
        "    return fft_result, spatial_result\n",
        "\n",
        "def illumination_pattern(angle, frequency, phase_offset, amplitude, size) -> np.ndarray:\n",
        "    n = size // 2\n",
        "    Y, X = np.mgrid[-n:n, -n:n]\n",
        "    ky, kx = np.sin(angle) * frequency, np.cos(angle) * frequency\n",
        "    return 1 + amplitude * np.cos(2 * np.pi * (X * kx + Y * ky) + phase_offset)\n",
        "\n",
        "\n",
        "class PerlinNoise:\n",
        "    \"\"\"Adapted from: https://github.com/pvigier/perlin-numpy\"\"\"\n",
        "\n",
        "    def __init__(self, size: int, res: int):\n",
        "        \"\"\"\n",
        "        :param size: length of both dimensions of the generated noise image\n",
        "        :param res: number of periods of noise to generate along each axis\n",
        "        \"\"\"\n",
        "\n",
        "        meshgrid = np.mgrid[0:res:res / size, 0:res:res / size]\n",
        "        self.grid = np.stack(meshgrid) % 1\n",
        "\n",
        "        self.t = self._fade(self.grid)\n",
        "        self.d = size // res\n",
        "        self.sample_size = (res + 1, res + 1)\n",
        "\n",
        "    def __call__(self) -> np.array:\n",
        "        \"\"\"\n",
        "        Returned values are always in the [0, 1] range with ~0.5 mean\n",
        "        \"\"\"\n",
        "        angles = 2 * np.pi * np.random.random_sample(self.sample_size)\n",
        "        gradients = np.dstack((np.cos(angles), np.sin(angles))).repeat(self.d, 0).repeat(self.d, 1)\n",
        "\n",
        "        n00 = (np.dstack((self.grid[0], self.grid[1])) * gradients[:-self.d, :-self.d]).sum(2)\n",
        "        n10 = (np.dstack((self.grid[0] - 1, self.grid[1])) * gradients[self.d:, :-self.d]).sum(2)\n",
        "        n01 = (np.dstack((self.grid[0], self.grid[1] - 1)) * gradients[:-self.d, self.d:]).sum(2)\n",
        "        n11 = (np.dstack((self.grid[0] - 1, self.grid[1] - 1)) * gradients[self.d:, self.d:]).sum(2)\n",
        "\n",
        "        n0 = n00 * (1 - self.t[0]) + n10 * self.t[0]\n",
        "        n1 = n01 * (1 - self.t[0]) + n11 * self.t[0]\n",
        "\n",
        "        return 0.5 + 2 ** -0.5 * (n0 * (1 - self.t[1]) + n1 * self.t[1])\n",
        "\n",
        "    @staticmethod\n",
        "    def _fade(t):\n",
        "        return 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3\n",
        "\n",
        "\n",
        "class SyntheticDataset(IterableDataset):\n",
        "    def __init__(self, contrast_fg_range: tuple[float,float] = (0.0, 1.0), contrast_bg_range: tuple[float,float] = (0.0, 1.0)):\n",
        "        self.patch_size = 128\n",
        "\n",
        "        self.contrast_fg_range = contrast_fg_range\n",
        "        self.contrast_bg_range = contrast_bg_range\n",
        "\n",
        "        self.frequency = 0.17\n",
        "        self.amplitude = 1.0\n",
        "\n",
        "        self.config = {\n",
        "            \"na\": 1.49,\n",
        "            \"wavelength\": 512,\n",
        "            \"px_size\": 0.07,\n",
        "            \"wiener_parameter\": 0.1,\n",
        "            \"apo_cutoff\": 2.0,\n",
        "            \"apo_bend\": 0.9\n",
        "        }\n",
        "\n",
        "        self.otf = OTF(self.config['na'], self.config['wavelength'], self.config['px_size'], self.patch_size // 2, 0.3)\n",
        "        self.otf_mult = self.otf(self.patch_size)\n",
        "\n",
        "        self.perlin = PerlinNoise(self.patch_size, 1)\n",
        "\n",
        "    def _simulate_sim(self, image):\n",
        "        angle0 = np.random.uniform(0, np.pi * 2)\n",
        "        phase_offsets = np.random.uniform(0, np.pi * 2, 3)\n",
        "\n",
        "        shifts = [(self.frequency * self.patch_size * np.sin(angle0 + i * np.pi / 3),\n",
        "                   self.frequency * self.patch_size * np.cos(angle0 + i * np.pi / 3))\n",
        "                  for i in range(3)]\n",
        "\n",
        "        illumination = np.stack([illumination_pattern(angle0 + i // 3 * np.pi / 3,\n",
        "                                                      self.frequency,\n",
        "                                                      phase_offsets[i // 3] + (i % 3) * np.pi * 2 / 3,\n",
        "                                                      self.amplitude,\n",
        "                                                      self.patch_size)\n",
        "                                 for i in range(9)])\n",
        "\n",
        "\n",
        "        fg_c = np.random.uniform(*self.contrast_fg_range)\n",
        "        bg_c = np.random.uniform(*self.contrast_bg_range)\n",
        "        foreground = 250 + fg_c * 500\n",
        "        background = 50 + bg_c * 50\n",
        "\n",
        "        high_res_image = (image * foreground + background) * self.perlin()\n",
        "\n",
        "        ix = np.fft.fft2(illumination * high_res_image)\n",
        "        hix = self.otf_mult * ix\n",
        "        dhix = hix.reshape(9, 2, self.patch_size // 2, 2, self.patch_size // 2).sum((1, 3)) / 4\n",
        "        low_res_images = np.random.poisson(np.fft.ifft2(dhix).real).astype(np.float64)\n",
        "\n",
        "        noisy_shifts = [np.random.triangular((y - 0.25, x - 0.25), (y, x), (y + 0.25, x + 0.25)) for y, x in shifts]\n",
        "        noisy_phase_offsets = np.random.normal(phase_offsets, np.pi / 6)\n",
        "        noisy_amplitudes = np.random.normal(self.amplitude, 0.1, 3)\n",
        "\n",
        "        reconstruction = run_reconstruction(np.fft.fft2(low_res_images), self.otf, noisy_shifts, noisy_phase_offsets, noisy_amplitudes, self.config)[1]\n",
        "        return (reconstruction - np.mean(reconstruction)) / np.std(reconstruction)\n",
        "\n",
        "class SyntheticCCPDataset(SyntheticDataset):\n",
        "    def __init__(self,min_n: int = 5, max_n: int = 15, radius: float = 2.5,\n",
        "        contrast_fg_range: tuple[float,float] = (0.0, 1.0),\n",
        "        contrast_bg_range: tuple[float,float] = (0.0, 1.0)):\n",
        "        super().__init__(\n",
        "            contrast_fg_range=contrast_fg_range,\n",
        "            contrast_bg_range=contrast_bg_range\n",
        "        )\n",
        "\n",
        "        self.patch_size = 128\n",
        "\n",
        "        # Possible positions\n",
        "        yy, xx = np.mgrid[15:self.patch_size - 1:16, 15:self.patch_size - 1:16]\n",
        "        self.yy = yy.flatten()\n",
        "        self.xx = xx.flatten()\n",
        "\n",
        "        # validate\n",
        "        if not (0 <= min_n < max_n <= 49):\n",
        "            raise ValueError(\"Require 0 <= min_n < max_n <= 49\")\n",
        "        self.min_n, self.max_n = min_n, max_n\n",
        "\n",
        "        self.max_offset = 8\n",
        "        assert self.max_offset < 16\n",
        "\n",
        "        # Beta params\n",
        "        self.beta_a = 2\n",
        "        self.beta_b = 1\n",
        "\n",
        "        # CCP shape params\n",
        "        self.radius = radius\n",
        "        self.thickness = 1.0\n",
        "\n",
        "        # Patch positions\n",
        "        self.yyy, self.xxx = np.mgrid[:self.patch_size, :self.patch_size]\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            yield self.data_sample()\n",
        "\n",
        "    def data_sample(self):\n",
        "        # Generate positions and classes\n",
        "        n = np.random.randint(self.min_n, self.max_n)\n",
        "        indices = np.random.choice(len(self.yy), size=n, replace=False)\n",
        "        offsets = np.random.uniform(-self.max_offset, self.max_offset, (n, 2))\n",
        "        positions = np.column_stack([self.yy[indices], self.xx[indices]]) + offsets\n",
        "        classes = np.random.beta(self.beta_a, self.beta_b, n) * 0.9 + 0.1\n",
        "\n",
        "        # Generate simulated HR image and output target\n",
        "        target_distance = classes * self.radius\n",
        "        distance = np.hypot(self.yyy[..., None] - positions[:, 0], self.xxx[..., None] - positions[:, 1])\n",
        "        abs_distance = np.abs(distance - target_distance)\n",
        "        parts = np.where(abs_distance > self.thickness, 0, np.log(np.interp(abs_distance / self.thickness, [0, 1], [np.e, 1])))\n",
        "        full_image = np.sum(parts, -1)\n",
        "\n",
        "        distances = np.maximum(classes - distance / ((1 - classes) * 2 + self.radius + self.thickness * 2), 0)\n",
        "        y = np.minimum(np.sum(distances, -1), 1)\n",
        "\n",
        "        # Generate simulated SIM image\n",
        "        x = super()._simulate_sim(full_image)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "def show_tracking(data, image_stack,\n",
        "                  y_min=512, y_max=768, x_min=256, x_max=512,\n",
        "                  tail_length=10, color='yellow', show_roi=True):\n",
        "    \"\"\"\n",
        "    Visualize CCP trajectories within a defined ROI, from either a CSV file or a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        data (str | pd.DataFrame): Path to CSV file or a DataFrame containing trajectories.\n",
        "        image_stack (np.ndarray): 3D numpy array (frames, height, width).\n",
        "        y_min, y_max, x_min, x_max (int): ROI bounds.\n",
        "        tail_length (int): Number of frames for trajectory tails.\n",
        "        color (str): Trajectory color.\n",
        "        show_roi (bool): Whether to display cyan ROI rectangle on the full frame.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(data, str):\n",
        "        trajectories_df = pd.read_csv(data)\n",
        "    elif isinstance(data, pd.DataFrame):\n",
        "        trajectories_df = data.copy()\n",
        "    else:\n",
        "        raise TypeError(\"`data` must be a CSV file path or a pandas DataFrame.\")\n",
        "\n",
        "    tracks_in_roi = trajectories_df.groupby('track_id').filter(\n",
        "        lambda t: (y_min < t.y.mean() < y_max) and (x_min < t.x.mean() < x_max)\n",
        "    )\n",
        "\n",
        "    html_code_linking = loading_html(\"Loading cropped region and tracks, please wait...\")\n",
        "    display(HTML(html_code_linking))\n",
        "\n",
        "    if show_roi:\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        ax.imshow(image_stack[0], cmap='magma')\n",
        "        rect = Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                         linewidth=2, edgecolor='cyan', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.set_title(\"Full image (cyan box shows cropped region)\")\n",
        "        plt.show()\n",
        "\n",
        "    def animate_trajectories_cropped(trajectories_df, image_stack, tail_length=10, color='yellow'):\n",
        "        cropped_stack = image_stack[:, y_min:y_max, x_min:x_max]\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(cropped_stack[0], cmap='magma')\n",
        "        particles = trajectories_df['track_id'].unique()\n",
        "\n",
        "        line_collections = {pid: mc.LineCollection([], linewidths=1, colors=color) for pid in particles}\n",
        "        for lc in line_collections.values():\n",
        "            ax.add_collection(lc)\n",
        "\n",
        "        dot = ax.scatter([], [], s=5, c=color)\n",
        "\n",
        "        def animate(i):\n",
        "            im.set_array(cropped_stack[i])\n",
        "\n",
        "            window = trajectories_df[\n",
        "                (trajectories_df['frame'] >= i - tail_length) &\n",
        "                (trajectories_df['frame'] <= i)\n",
        "            ]\n",
        "\n",
        "            now = window[window['frame'] == i]\n",
        "            if len(now) > 0:\n",
        "                coords = np.column_stack((now.x.values - x_min, now.y.values - y_min))\n",
        "                dot.set_offsets(coords)\n",
        "            else:\n",
        "                dot.set_offsets(np.empty((0, 2)))\n",
        "\n",
        "            for pid in particles:\n",
        "                traj = window[window['track_id'] == pid].sort_values('frame')\n",
        "                if len(traj) >= 2:\n",
        "                    segs = [\n",
        "                        [(x0 - x_min, y0 - y_min), (x1 - x_min, y1 - y_min)]\n",
        "                        for (x0, y0, x1, y1) in zip(\n",
        "                            traj.x.values[:-1], traj.y.values[:-1],\n",
        "                            traj.x.values[1:], traj.y.values[1:]\n",
        "                        )\n",
        "                    ]\n",
        "                    line_collections[pid].set_segments(segs)\n",
        "                else:\n",
        "                    line_collections[pid].set_segments([])\n",
        "\n",
        "            return [im, dot] + list(line_collections.values())\n",
        "\n",
        "        ani = FuncAnimation(fig, animate, frames=cropped_stack.shape[0], interval=100, blit=True)\n",
        "        plt.close(fig)\n",
        "        return HTML(ani.to_jshtml())\n",
        "\n",
        "    html = animate_trajectories_cropped(tracks_in_roi, image_stack, tail_length, color)\n",
        "    display(html)\n",
        "    display(HTML(replace_loading_js_empty))\n",
        "\n",
        "    print(\"Total number of trajectories in ROI:\", len(tracks_in_roi['track_id'].unique()))\n",
        "\n",
        "\n",
        "def show_tracking_compare(\n",
        "    data_left, data_right,\n",
        "    image_stack,\n",
        "    label_left=\"Model A\",\n",
        "    label_right=\"Model B\",\n",
        "    color_left=\"yellow\",\n",
        "    color_right=\"cyan\",\n",
        "    y_min=512, y_max=768, x_min=256, x_max=512,\n",
        "    tail_length=10):\n",
        "\n",
        "    \"\"\"\n",
        "    Side-by-side comparison of two tracking results (animated).\n",
        "    Args:\n",
        "        data_left (pd.DataFrame): Tracking results for the left panel\n",
        "        data_right (pd.DataFrame): Tracking results for the right panel\n",
        "        image_stack (np.ndarray): Image sequence.\n",
        "        label_left (str): Title of the left panel.\n",
        "        label_right (str): Title of the right panel.\n",
        "        color_left (str): Trajectory color (left).\n",
        "        color_right (str): Trajectory color (right).\n",
        "        y_min, y_max (int): Vertical ROI bounds.\n",
        "        x_min, x_max (int): Horizontal ROI bounds.\n",
        "        tail_length (int): Number of frames shown in trajectory tails.\n",
        "\n",
        "    Returns:\n",
        "        HTML: Synchronized side-by-side animation.\n",
        "    \"\"\"\n",
        "    df_L = data_left.copy()\n",
        "    df_R = data_right.copy()\n",
        "\n",
        "    # Filter ROI\n",
        "    def filter_roi(df):\n",
        "        return df.groupby(\"track_id\").filter(\n",
        "            lambda t: (y_min < t.y.mean() < y_max) and (x_min < t.x.mean() < x_max)\n",
        "        )\n",
        "\n",
        "    df_L = filter_roi(df_L)\n",
        "    df_R = filter_roi(df_R)\n",
        "\n",
        "    cropped = image_stack[:, y_min:y_max, x_min:x_max]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    ims = []\n",
        "    dots = []\n",
        "    lcs = []\n",
        "\n",
        "    for ax, df, col, title in zip(\n",
        "        axes,\n",
        "        [df_L, df_R],\n",
        "        [color_left, color_right],\n",
        "        [label_left, label_right]\n",
        "    ):\n",
        "        im = ax.imshow(cropped[0], cmap=\"magma\")\n",
        "        ims.append(im)\n",
        "\n",
        "        pids = df[\"track_id\"].unique()\n",
        "        collections = {pid: mc.LineCollection([], linewidths=1, colors=col) for pid in pids}\n",
        "        for lc in collections.values():\n",
        "            ax.add_collection(lc)\n",
        "\n",
        "        dot = ax.scatter([], [], s=5, c=col)\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        dots.append(dot)\n",
        "        lcs.append((collections, df))\n",
        "\n",
        "    def animate(i):\n",
        "        artists = []\n",
        "        for k in range(2):\n",
        "            ims[k].set_array(cropped[i])\n",
        "            df = lcs[k][1]\n",
        "            collections = lcs[k][0]\n",
        "\n",
        "            window = df[(df.frame >= i - tail_length) & (df.frame <= i)]\n",
        "            now = window[window.frame == i]\n",
        "\n",
        "            if len(now):\n",
        "                dots[k].set_offsets(\n",
        "                    np.column_stack((now.x - x_min, now.y - y_min))\n",
        "                )\n",
        "            else:\n",
        "                dots[k].set_offsets(np.empty((0, 2)))\n",
        "\n",
        "            for pid, lc in collections.items():\n",
        "                traj = window[window.track_id == pid].sort_values(\"frame\")\n",
        "                if len(traj) >= 2:\n",
        "                    segs = [\n",
        "                        [(x0 - x_min, y0 - y_min), (x1 - x_min, y1 - y_min)]\n",
        "                        for x0, y0, x1, y1 in zip(\n",
        "                            traj.x[:-1], traj.y[:-1],\n",
        "                            traj.x[1:], traj.y[1:]\n",
        "                        )\n",
        "                    ]\n",
        "                    lc.set_segments(segs)\n",
        "                else:\n",
        "                    lc.set_segments([])\n",
        "\n",
        "            artists += [ims[k], dots[k]] + list(collections.values())\n",
        "\n",
        "        return artists\n",
        "\n",
        "    ani = FuncAnimation(fig, animate, frames=cropped.shape[0], interval=100, blit=True)\n",
        "    plt.close(fig)\n",
        "\n",
        "    return HTML(ani.to_jshtml())\n",
        "\n",
        "def visualize_single_sample_comparison(model,\n",
        "                                        img_tensor,\n",
        "                                        gt_mask_tensor,\n",
        "                                        device,\n",
        "                                        threshold=0.5,\n",
        "                                        sigma=1.0,\n",
        "                                        model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Visualizes the prediction of a single model on a specific sample.\n",
        "    Args:\n",
        "        model (torch.nn.Module): Trained segmentation/detection model.\n",
        "        img_tensor (torch.Tensor): Input image tensor (H, W).\n",
        "        gt_mask_tensor (torch.Tensor): Ground truth mask (H, W).\n",
        "        device (torch.device): Device used for inference.\n",
        "        threshold (float): Probability threshold for binary output.\n",
        "        sigma (float): Gaussian smoothing sigma (0 disables smoothing).\n",
        "        model_name (str): Label shown in the figure title.\n",
        "\n",
        "    Shows:\n",
        "        input image, ground truth mask, predicted probability map,\n",
        "        and thresholded binary mask.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    img_tensor = img_tensor.to(device)\n",
        "    gt_mask_tensor = gt_mask_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Run through model\n",
        "        logits = model(img_tensor.unsqueeze(0))\n",
        "        prob_map = torch.sigmoid(logits[0, 0]).cpu().numpy()\n",
        "\n",
        "    # Optional smoothing\n",
        "    if sigma > 0:\n",
        "        prob_map = gaussian_filter(prob_map, sigma=sigma)\n",
        "\n",
        "    # Threshold for binary detection\n",
        "    pred_mask = (prob_map >= threshold).astype(float)\n",
        "\n",
        "    img = img_tensor.squeeze(0).cpu().numpy()\n",
        "    gt_mask = gt_mask_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    fig.suptitle(f\"Srovn√°n√≠: {model_name}\", fontsize=16)\n",
        "\n",
        "    axes[0].imshow(img, cmap='magma')\n",
        "    axes[0].set_title(\"Input Image\")\n",
        "    axes[1].imshow(gt_mask, cmap='gray', vmin=0, vmax=1)\n",
        "    axes[1].set_title(\"Ground Truth Mask\")\n",
        "    axes[2].imshow(prob_map, cmap='viridis')\n",
        "    axes[2].set_title(\"Predicted Heatmap\")\n",
        "    axes[3].imshow(pred_mask, cmap='gray', vmin=0, vmax=1)\n",
        "    axes[3].set_title(f\"Thresholded Output (>{threshold})\")\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "def show_detection_compare(\n",
        "    detections_left,\n",
        "    detections_right,\n",
        "    image_stack,\n",
        "    label_left=\"Detector A\",\n",
        "    label_right=\"Detector B\",\n",
        "    color_left=\"yellow\",\n",
        "    color_right=\"cyan\",\n",
        "    y_min=512, y_max=768, x_min=256, x_max=512,\n",
        "    max_frames=None,\n",
        "    interval=100\n",
        "):\n",
        "    \"\"\"\n",
        "    Side-by-side comparison of two detection results (animated).\n",
        "\n",
        "    Args:\n",
        "        detections_left (list[list[tuple]]): Per-frame detections [(x, y)] ‚Äì left panel\n",
        "        detections_right (list[list[tuple]]): Per-frame detections [(x, y)] ‚Äì right panel\n",
        "        image_stack (np.ndarray): Image sequence (frames, H, W)\n",
        "        label_left (str): Title of the left panel\n",
        "        label_right (str): Title of the right panel\n",
        "        color_left (str): Detection color (left)\n",
        "        color_right (str): Detection color (right)\n",
        "        y_min, y_max, x_min, x_max (int): ROI bounds\n",
        "        max_frames (int or None): Limit number of frames (default = all)\n",
        "        interval (int): Animation interval in ms\n",
        "\n",
        "    Returns:\n",
        "        HTML: Side-by-side animation\n",
        "    \"\"\"\n",
        "\n",
        "    def to_df(detections):\n",
        "        rows = [\n",
        "            (frame, x, y)\n",
        "            for frame, dets in enumerate(detections)\n",
        "            for (x, y) in dets\n",
        "        ]\n",
        "        df = pd.DataFrame(rows, columns=[\"frame\", \"x\", \"y\"])\n",
        "        return df[\n",
        "            (df.y.between(y_min, y_max)) &\n",
        "            (df.x.between(x_min, x_max))\n",
        "        ]\n",
        "\n",
        "    df_L = to_df(detections_left)\n",
        "    df_R = to_df(detections_right)\n",
        "\n",
        "    cropped = image_stack[:, y_min:y_max, x_min:x_max]\n",
        "\n",
        "    n_frames = cropped.shape[0]\n",
        "    if max_frames is not None:\n",
        "        n_frames = min(n_frames, max_frames)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    ims = []\n",
        "    dots = []\n",
        "    dfs = [df_L, df_R]\n",
        "    colors = [color_left, color_right]\n",
        "    titles = [label_left, label_right]\n",
        "\n",
        "    for ax, df, col, title in zip(axes, dfs, colors, titles):\n",
        "        im = ax.imshow(cropped[0], cmap=\"magma\")\n",
        "        dot = ax.scatter([], [], s=10, c=col)\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        ims.append(im)\n",
        "        dots.append(dot)\n",
        "\n",
        "    def animate(i):\n",
        "        artists = []\n",
        "        for k in range(2):\n",
        "            ims[k].set_array(cropped[i])\n",
        "            now = dfs[k][dfs[k].frame == i]\n",
        "\n",
        "            if len(now):\n",
        "                coords = np.column_stack(\n",
        "                    (now.x.values - x_min, now.y.values - y_min)\n",
        "                )\n",
        "                dots[k].set_offsets(coords)\n",
        "            else:\n",
        "                dots[k].set_offsets(np.empty((0, 2)))\n",
        "\n",
        "            artists += [ims[k], dots[k]]\n",
        "\n",
        "        return artists\n",
        "\n",
        "    ani = FuncAnimation(\n",
        "        fig,\n",
        "        animate,\n",
        "        frames=n_frames,\n",
        "        interval=interval,\n",
        "        blit=True\n",
        "    )\n",
        "\n",
        "    plt.close(fig)\n",
        "    return HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "qsjWSR-4g_JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vyhodnocen√≠ HOTA\n",
        "\n",
        "#from baseline\n",
        "def hota(gt: pd.DataFrame, tr: pd.DataFrame, threshold: float = 5) -> dict[str, float]:\n",
        "    \"\"\"Slightly adapted from https://github.com/JonathonLuiten/TrackEval\"\"\"\n",
        "\n",
        "    # Ensure particle ids are sorted from 0 to max(n)\n",
        "    gt = gt.copy()\n",
        "    tr = tr.copy()\n",
        "\n",
        "    gt.track_id = gt.track_id.map({old: new for old, new in zip(gt.track_id.unique(), range(gt.track_id.nunique()))})\n",
        "    tr.track_id = tr.track_id.map({old: new for old, new in zip(tr.track_id.unique(), range(tr.track_id.nunique()))})\n",
        "\n",
        "    # Initialization\n",
        "    num_gt_ids = gt.track_id.nunique()\n",
        "    num_tr_ids = tr.track_id.nunique()\n",
        "\n",
        "    frames = sorted(set(gt.frame.unique()) | set(tr.frame.unique()))\n",
        "\n",
        "    potential_matches_count = np.zeros((num_gt_ids, num_tr_ids))\n",
        "    gt_id_count = np.zeros((num_gt_ids, 1))\n",
        "    tracker_id_count = np.zeros((1, num_tr_ids))\n",
        "\n",
        "    HOTA_TP, HOTA_FN, HOTA_FP = 0, 0, 0\n",
        "    LocA = 0.0\n",
        "\n",
        "    # Compute similarities (inverted normalized distance)\n",
        "    similarities = [1 - np.clip(spatial.distance.cdist(gt[gt.frame == t][['x', 'y']],\n",
        "                                                       tr[tr.frame == t][['x', 'y']]) / threshold, 0, 1)\n",
        "                    for t in frames]\n",
        "\n",
        "    # Accumulate global track information\n",
        "    for t in frames:\n",
        "        gt_ids_t = gt[gt.frame == t].track_id.to_numpy()\n",
        "        tr_ids_t = tr[tr.frame == t].track_id.to_numpy()\n",
        "\n",
        "        similarity = similarities[t]\n",
        "        sim_iou_denom = similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n",
        "        sim_iou = np.zeros_like(similarity)\n",
        "        sim_iou_mask = sim_iou_denom > 0 + np.finfo('float').eps\n",
        "        sim_iou[sim_iou_mask] = similarity[sim_iou_mask] / sim_iou_denom[sim_iou_mask]\n",
        "        potential_matches_count[gt_ids_t[:, None], tr_ids_t[None, :]] += sim_iou\n",
        "\n",
        "        gt_id_count[gt_ids_t] += 1\n",
        "        tracker_id_count[0, tr_ids_t] += 1\n",
        "\n",
        "    global_alignment_score = potential_matches_count / (gt_id_count + tracker_id_count - potential_matches_count)\n",
        "    matches_count = np.zeros_like(potential_matches_count)\n",
        "\n",
        "    # Calculate scores for each timestep\n",
        "    for t in frames:\n",
        "        gt_ids_t = gt[gt.frame == t].track_id.to_numpy()\n",
        "        tr_ids_t = tr[tr.frame == t].track_id.to_numpy()\n",
        "\n",
        "        if len(gt_ids_t) == 0:\n",
        "            HOTA_FP += len(tr_ids_t)\n",
        "            continue\n",
        "\n",
        "        if len(tr_ids_t) == 0:\n",
        "            HOTA_FN += len(gt_ids_t)\n",
        "            continue\n",
        "\n",
        "        similarity = similarities[t]\n",
        "        score_mat = global_alignment_score[gt_ids_t[:, None], tr_ids_t[None, :]] * similarity\n",
        "\n",
        "        match_rows, match_cols = optimize.linear_sum_assignment(-score_mat)\n",
        "\n",
        "        actually_matched_mask = similarity[match_rows, match_cols] > 0\n",
        "        alpha_match_rows = match_rows[actually_matched_mask]\n",
        "        alpha_match_cols = match_cols[actually_matched_mask]\n",
        "\n",
        "        num_matches = len(alpha_match_rows)\n",
        "\n",
        "        HOTA_TP += num_matches\n",
        "        HOTA_FN += len(gt_ids_t) - num_matches\n",
        "        HOTA_FP += len(tr_ids_t) - num_matches\n",
        "\n",
        "        if num_matches > 0:\n",
        "            LocA += sum(similarity[alpha_match_rows, alpha_match_cols])\n",
        "            matches_count[gt_ids_t[alpha_match_rows], tr_ids_t[alpha_match_cols]] += 1\n",
        "\n",
        "    ass_a = matches_count / np.maximum(1, gt_id_count + tracker_id_count - matches_count)\n",
        "    AssA = np.sum(matches_count * ass_a) / np.maximum(1, HOTA_TP)\n",
        "    DetA = HOTA_TP / np.maximum(1, HOTA_TP + HOTA_FN + HOTA_FP)\n",
        "    HOTA = np.sqrt(DetA * AssA)\n",
        "\n",
        "    return {'HOTA': HOTA, 'AssA': AssA, 'DetA': DetA, 'LocA': LocA,\n",
        "            'HOTA TP': HOTA_TP, 'HOTA FN': HOTA_FN, 'HOTA FP': HOTA_FP}\n",
        "\n",
        "\n",
        "def calculate_performance(gt_path, tracks,\n",
        "                          y_min=512, y_max=768, x_min=256, x_max=512,\n",
        "                          name=\"Method\"):\n",
        "    \"\"\"\n",
        "    Calculate and print HOTA-based performance metrics for a tracking result.\n",
        "\n",
        "    Args:\n",
        "        gt_path (str): Path to ground truth CSV file.\n",
        "        tracks (str | pd.DataFrame): Tracking results (as DataFrame or CSV path).\n",
        "        y_min, y_max, x_min, x_max (int): ROI bounds.\n",
        "        name (str): Name of the method (for display).\n",
        "    \"\"\"\n",
        "\n",
        "    val_gt = pd.read_csv(gt_path)\n",
        "    val_gt = val_gt.groupby('track_id').filter(\n",
        "        lambda t: (y_min < t.y.mean() < y_max) and (x_min < t.x.mean() < x_max)\n",
        "    )\n",
        "\n",
        "    if isinstance(tracks, str):\n",
        "        val_tracks = pd.read_csv(tracks)\n",
        "    elif isinstance(tracks, pd.DataFrame):\n",
        "        val_tracks = tracks.copy()\n",
        "    else:\n",
        "        raise TypeError(\"`tracks` must be a CSV path or a pandas DataFrame.\")\n",
        "\n",
        "    val_tracks = val_tracks.groupby('track_id').filter(\n",
        "        lambda t: (y_min < t.y.mean() < y_max) and (x_min < t.x.mean() < x_max)\n",
        "    )\n",
        "\n",
        "    # Compute metrics\n",
        "    results = hota(val_gt, val_tracks)\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  HOTA: {results['HOTA']:.2f} \"\n",
        "          f\"(AssA: {results['AssA']:.2f}, DetA: {results['DetA']:.2f})\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "    print(f\"‚úÖ Done\")"
      ],
      "metadata": {
        "id": "_4jvCeM0g_MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pytorch Dataset\n",
        "\n",
        "#from baseline\n",
        "class SyntheticCCPDatasetTorch(torch_data.Dataset):\n",
        "    \"\"\"PyTorch Dataset that yields synthetic CCP images and masks. \"\"\"\n",
        "    # Changing these parameters too much could result in data that doesn‚Äôt match the real validation or test set.\n",
        "    _min_n = 5\n",
        "    _max_n = 15\n",
        "    _radius = 2.5\n",
        "    _contrast_fg_range = (0.0, 1.0)\n",
        "    _contrast_bg_range = (0.0, 1.0)\n",
        "\n",
        "    def __init__(self, length: int = 500):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        # create one generator with the fixed parameters\n",
        "        self._synthetic = SyntheticCCPDataset(\n",
        "            min_n=self._min_n,\n",
        "            max_n=self._max_n,\n",
        "            radius=self._radius,\n",
        "            contrast_fg_range=self._contrast_fg_range,\n",
        "            contrast_bg_range=self._contrast_bg_range,\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # draw a new synthetic sample\n",
        "        img, mask = self._synthetic.data_sample()\n",
        "\n",
        "        # Normalise image to [0, 1] for better training stability\n",
        "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors and add channel dimension\n",
        "        img_tensor = torch.from_numpy(img).unsqueeze(0).float()\n",
        "        mask_tensor = torch.from_numpy(mask).unsqueeze(0).float()\n",
        "\n",
        "        return img_tensor, mask_tensor"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sihZbaAthfvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Predikce model≈Ø\n",
        "\n",
        "\n",
        "def predict_ccps_with_model(model: nn.Module,\n",
        "                            image: np.ndarray,\n",
        "                            device: torch.device,\n",
        "                            threshold: float = 0.2, #p≈Øvodnƒõ 0.5\n",
        "                            sigma: float = 0.5) -> list[tuple[int, int]]: #p≈Øvodƒõ 1\n",
        "    \"\"\"Use a trained model to detect CCPs in a single image.\n",
        "\n",
        "    Args:\n",
        "        model: trained CCPDetectorNet.\n",
        "        image: 2‚ÄëD numpy array.\n",
        "        device: torch device for inference.\n",
        "        threshold: probability threshold for peak selection.\n",
        "        sigma: optional Gaussian smoothing applied on the model output\n",
        "                to reduce noise before local maxima detection.\n",
        "\n",
        "    Returns:\n",
        "        A list of (x, y) tuples indicating detected CCP positions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # normalise image to [0,1]\n",
        "        img_norm = (image - image.min()) / (image.max() - image.min() + 1e-6)\n",
        "        tensor = torch.from_numpy(img_norm).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "        logits = model(tensor)\n",
        "        prob_map = torch.sigmoid(logits[0, 0]).cpu().numpy()\n",
        "    # optional smoothing\n",
        "    if sigma > 0:\n",
        "        prob_map = gaussian_filter(prob_map, sigma=sigma)\n",
        "        #prob_map = median_filter(prob_map, size = 3) #it was tried, but it gave worse results\n",
        "\n",
        "    # threshold\n",
        "    mask = prob_map >= threshold\n",
        "    # local maxima in 3√ó3 neighbourhood\n",
        "    local_max = prob_map == maximum_filter(prob_map, size=3)\n",
        "    peaks = np.argwhere(local_max & mask)\n",
        "    detections = [(int(col), int(row)) for row, col in peaks]\n",
        "    return detections"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cnUy7qqnhi3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Detektor\n",
        "def run_detector(model, device, tif_path=\"val_data/val.tif\",\n",
        "                 threshold=0.2, sigma=0.5):\n",
        "    \"\"\"\n",
        "    Runs the detector on the entire TIFF sequence and returns detections for each frame.\n",
        "\n",
        "    Args:\n",
        "      model: trained model (UNet, ResUNet, ‚Ä¶)\n",
        "      device: 'cpu' or 'cuda'\n",
        "      tif_path: path to the TIFF containing the time sequence\n",
        "      threshold: detection threshold\n",
        "      sigma: smoothing sigma\n",
        "    Returns:\n",
        "      List[list[(x, y)]] ‚Äî detections for each frame\n",
        "      input_stack ‚Äî the loaded TIFF for further use.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load val.tif\n",
        "    input_stack = open_tiff_file(tif_path).astype(np.float64)\n",
        "\n",
        "    detections_per_frame = []\n",
        "    num_frames = input_stack.shape[0]\n",
        "\n",
        "    for frame_idx in range(num_frames):\n",
        "        frame = input_stack[frame_idx]\n",
        "        dets = predict_ccps_with_model(\n",
        "            model,\n",
        "            frame,\n",
        "            device,\n",
        "            threshold=threshold,\n",
        "            sigma=sigma\n",
        "        )\n",
        "        detections_per_frame.append(dets)\n",
        "\n",
        "    print(f\"‚úÖ Detection complete. Frames processed: {num_frames}\")\n",
        "\n",
        "    return detections_per_frame, input_stack"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8-ltgxuEhi8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tracking"
      ],
      "metadata": {
        "id": "wmOyGtAnhoCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Detektor\n",
        "def run_detector(model, device, tif_path=\"val_data/val.tif\",\n",
        "                 threshold=0.2, sigma=0.5):\n",
        "    \"\"\"\n",
        "    Runs the detector on the entire TIFF sequence and returns detections for each frame.\n",
        "\n",
        "    Args:\n",
        "      model: trained model (UNet, ResUNet, ‚Ä¶)\n",
        "      device: 'cpu' or 'cuda'\n",
        "      tif_path: path to the TIFF containing the time sequence\n",
        "      threshold: detection threshold\n",
        "      sigma: smoothing sigma\n",
        "    Returns:\n",
        "      List[list[(x, y)]] ‚Äî detections for each frame\n",
        "      input_stack ‚Äî the loaded TIFF for further use.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load val.tif\n",
        "    input_stack = open_tiff_file(tif_path).astype(np.float64)\n",
        "\n",
        "    detections_per_frame = []\n",
        "    num_frames = input_stack.shape[0]\n",
        "\n",
        "    for frame_idx in range(num_frames):\n",
        "        frame = input_stack[frame_idx]\n",
        "        dets = predict_ccps_with_model(\n",
        "            model,\n",
        "            frame,\n",
        "            device,\n",
        "            threshold=threshold,\n",
        "            sigma=sigma\n",
        "        )\n",
        "        detections_per_frame.append(dets)\n",
        "\n",
        "    print(f\"‚úÖ Detection complete. Frames processed: {num_frames}\")\n",
        "\n",
        "    return detections_per_frame, input_stack"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yogFLXN0hjBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tracker Kalman≈Øv filtr + Hungarian algoritmus\n",
        "\n",
        "# Track object\n",
        "class Track:\n",
        "    def __init__(self, x, y, track_id):\n",
        "        self.kf = KalmanFilter(dim_x=4, dim_z=2)\n",
        "        self.kf.F = np.array([[1,0,1,0],\n",
        "                              [0,1,0,1],\n",
        "                              [0,0,1,0],\n",
        "                              [0,0,0,1]])\n",
        "        self.kf.H = np.array([[1,0,0,0],\n",
        "                              [0,1,0,0]])\n",
        "        self.kf.R *= 0.5\n",
        "        self.kf.P *= 10\n",
        "\n",
        "        # initial position\n",
        "        self.kf.x[:2] = np.array([[x],[y]])\n",
        "\n",
        "        self.id = track_id\n",
        "        self.time_since_update = 0\n",
        "\n",
        "    def predict(self):\n",
        "        self.kf.predict()\n",
        "        x, y = self.kf.x[0][0], self.kf.x[1][0]\n",
        "        return (x, y)\n",
        "\n",
        "    def update(self, z):\n",
        "        self.kf.update(z)\n",
        "        self.time_since_update = 0\n",
        "\n",
        "\n",
        "# Hungarian algoritm\n",
        "def hungarian_association(tracks, detections, max_dist=6):\n",
        "\n",
        "  \"\"\"\n",
        "    max_dist = maximum object displacement between frames\n",
        "    tracks = list of Track objects\n",
        "    detections = list of (x, y) tuples\n",
        "  \"\"\"\n",
        "\n",
        "  if len(tracks)==0:\n",
        "        # no existing tracks ‚Üí everything is new\n",
        "        return [], list(range(len(detections))), []\n",
        "\n",
        "  if len(detections)==0:\n",
        "        # no detections ‚Üí all tracks unmatched\n",
        "        return list(range(len(tracks))), [], []\n",
        "\n",
        "  cost = np.zeros((len(tracks), len(detections)))\n",
        "\n",
        "    # compute distance matrix\n",
        "  for i, t in enumerate(tracks):\n",
        "        tx, ty = t.predict()\n",
        "        for j, (x, y) in enumerate(detections):\n",
        "            cost[i, j] = np.hypot(tx - x, ty - y) #eucleidian\n",
        "\n",
        "  row_ind, col_ind = linear_sum_assignment(cost)\n",
        "\n",
        "  matches = []\n",
        "  unmatched_tracks = list(range(len(tracks)))\n",
        "  unmatched_detections = list(range(len(detections)))\n",
        "\n",
        "  for r, c in zip(row_ind, col_ind):\n",
        "        if cost[r, c] < max_dist:\n",
        "            matches.append((r, c))\n",
        "            unmatched_tracks.remove(r)\n",
        "            unmatched_detections.remove(c)\n",
        "\n",
        "  return unmatched_tracks, unmatched_detections, matches\n",
        "\n",
        "\n",
        "# tracking loop\n",
        "def track_ccp_sequence(detections_per_frame, max_dist):\n",
        "    tracks = []\n",
        "    next_id = 0\n",
        "    records = []\n",
        "\n",
        "    for frame, detections in enumerate(detections_per_frame):\n",
        "\n",
        "        # pre-prediction for all tracks\n",
        "        _ = [t.predict() for t in tracks]\n",
        "\n",
        "        unmatched_tracks, unmatched_dets, matches = hungarian_association(tracks, detections, max_dist)\n",
        "\n",
        "        # update matched tracks\n",
        "        for t_idx, d_idx in matches:\n",
        "            x, y = detections[d_idx]\n",
        "            tracks[t_idx].update(np.array([x, y]))\n",
        "            records.append({\n",
        "                \"frame\": frame,\n",
        "                \"x\": x,\n",
        "                \"y\": y,\n",
        "                \"track_id\": tracks[t_idx].id\n",
        "            })\n",
        "\n",
        "        # new tracks for unmatched detections\n",
        "        for d_idx in unmatched_dets:\n",
        "            x, y = detections[d_idx]\n",
        "            new_track = Track(x, y, next_id)\n",
        "            next_id += 1\n",
        "            tracks.append(new_track)\n",
        "            records.append({\n",
        "                \"frame\": frame,\n",
        "                \"x\": x,\n",
        "                \"y\": y,\n",
        "                \"track_id\": new_track.id\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(records)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "buZfpKCWhq4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Noirfar tracker\n",
        "\n",
        "def track_with_norfair(detections_per_frame, max_dist=8):\n",
        "    \"\"\"\n",
        "    Track objects using the Norfair multi-object tracker..\n",
        "    \"\"\"\n",
        "    # Initialize Norfair tracker with L1 (Manhattan) distance\n",
        "    tracker = Tracker(\n",
        "    distance_function=\"cityblock\",\n",
        "    distance_threshold=max_dist\n",
        ")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for frame_idx, dets in enumerate(detections_per_frame):\n",
        "\n",
        "        # Create Norfair detections\n",
        "        norfair_dets = [\n",
        "            Detection(points=np.array([[x, y]], dtype=float))\n",
        "            for (x, y) in dets\n",
        "        ]\n",
        "\n",
        "        # Update tracker\n",
        "        tracked = tracker.update(norfair_dets)\n",
        "\n",
        "        # Save output\n",
        "        for t in tracked:\n",
        "            x, y = t.estimate[0]  # one point\n",
        "\n",
        "            results.append({\n",
        "                \"frame\": frame_idx,\n",
        "                \"x\": float(x),\n",
        "                \"y\": float(y),\n",
        "                \"track_id\": int(t.id)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S6lPmOKohq_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grid-search pro parametry\n",
        "\n",
        "thresholds = [ 0.1, 0.5]# 0.2, 0.5]  # Detection probability thresholds\n",
        "sigmas = [0.1, 0.5,1] # Gaussian smoothing sigmas\n",
        "max_dist_values = [ 6, 8] # Max allowed displacement for tracking\n",
        "input = open_tiff_file(\"val_data/val.tif\").astype(np.float64)\n",
        "\n",
        "def run_grid_search(model, track_method):\n",
        "    \"\"\"\n",
        "    Run a grid search over detection and tracking parameters\n",
        "    and evaluate tracking performance on validation data.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Trained detection model.\n",
        "        track_method (str): Tracking backend (\"nn\", \"kalman\", or \"norfair\").\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Table with tracking metrics for each parameter combination.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Running grid-search with method {track_method}\")\n",
        "    results = []\n",
        "\n",
        "    for md in max_dist_values:\n",
        "        for th in thresholds:\n",
        "            for sg in sigmas:\n",
        "                print(f\"Testing max_dist = {md} ...\")\n",
        "                print(f\"Testing threshold={th}, sigma={sg} ...\")\n",
        "\n",
        "                # 1) Detection\n",
        "                detections_per_frame = []\n",
        "                for frame in range(len(input)):\n",
        "                    dets = predict_ccps_with_model(\n",
        "                        model, input[frame], device,\n",
        "                        threshold=th,\n",
        "                        sigma=sg\n",
        "                    )\n",
        "                    detections_per_frame.append(dets)\n",
        "\n",
        "                # 2) Tracking\n",
        "                if track_method == \"nn\":\n",
        "                   tracks = link_detections(detections_per_frame, max_dist=md)\n",
        "                elif track_method == \"kalman\":\n",
        "                   tracks =track_ccp_sequence(detections_per_frame, max_dist=md)\n",
        "                elif track_method == \"norfair\":\n",
        "                   tracks = track_with_norfair(detections_per_frame, max_dist=md)\n",
        "\n",
        "                # 3) Evaluation\n",
        "                metrics = calculate_performance(\n",
        "                    \"val_data/val.csv\",\n",
        "                    tracks,\n",
        "                    name=\"temp\"\n",
        "                )\n",
        "                results.append({\n",
        "                    \"threshold\": th,\n",
        "                    \"sigma\": sg,\n",
        "                    \"max_dist\": md,\n",
        "                    \"HOTA\": metrics[\"HOTA\"],\n",
        "                    \"AssA\": metrics[\"AssA\"],\n",
        "                    \"DetA\": metrics[\"DetA\"],\n",
        "                })\n",
        "    df = pd.DataFrame(results)\n",
        "    display(df)\n",
        "    return df"
      ],
      "metadata": {
        "id": "B0UBju83hrGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Heat map\n",
        "\n",
        "#haet map\n",
        "def plot_heatmap(df, metric=\"HOTA\", max_dist=6, model_name=\"unet\"):\n",
        "    \"\"\"\n",
        "    Heatmap threshold √ó sigma for fixed max_dist.\n",
        "    \"\"\"\n",
        "    df_f = df[df[\"max_dist\"] == max_dist]\n",
        "\n",
        "    pivot = df_f.pivot(\n",
        "        index=\"threshold\",\n",
        "        columns=\"sigma\",\n",
        "        values=metric\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
        "    plt.title(f\"{metric} heatmap (threshold √ó sigma), max_dist={max_dist}\")\n",
        "    plt.xlabel(\"Sigma\")\n",
        "    plt.ylabel(\"Threshold\")\n",
        "\n",
        "    filename = f\"heatmap_{model_name}_{metric}_md{max_dist}.pdf\"\n",
        "    plt.savefig(filename, bbox_inches=\"tight\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Kdzp27j1h5qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Nahr√°n√≠ natr√©novan√Ωch model≈Ø\n",
        "\n",
        "!gdown --id 1RESdDlQCMGcceWJOE31g3wNpzKVoE4aB -O unet_trained.pth\n",
        "!gdown --id 1I1tv1Qv-Z0u7lrrnLIyaS6EX5Ad0wbdE -O resunet_trained.pth"
      ],
      "metadata": {
        "id": "elwV2jImh_St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Naƒçten√≠ v√Ωsledk≈Ø model≈Ø\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_unet = UNet()\n",
        "state1 = torch.load(\"unet_trained.pth\", map_location=device, weights_only=False)\n",
        "model_unet.load_state_dict(state1)\n",
        "model_unet.to(device)\n",
        "model_unet.eval()\n",
        "\n",
        "model_resunet = ResUNet()\n",
        "state2 = torch.load(\"resunet_trained.pth\", map_location=device, weights_only=False)\n",
        "model_resunet.load_state_dict(state2)\n",
        "model_resunet.to(device)\n",
        "model_resunet.eval()\n",
        "\n",
        "print(\"üî• Models successfully loaded and ready!\")"
      ],
      "metadata": {
        "id": "y7eX6OG3h_eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vyhodnocen√≠ U-Net\n",
        "Parametry p≈ôi tr√©nov√°n√≠ modelu U-net:\n",
        "- Velikost datasetu 4000\n",
        "- Batch size = 32\n",
        "- Poƒçet epoch = 20\n",
        "- Learning rate = 0.001"
      ],
      "metadata": {
        "id": "6Y3e8uCaiEx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ladƒõn√≠ parametr≈Ø\n",
        "\n",
        "#run grid search and plot heat map for each max_dist\n",
        "df = run_grid_search(model_unet, track_method = \"nn\")\n",
        "for md in [ 6, 8]:\n",
        "    plot_heatmap(df, metric=\"HOTA\", max_dist=md)"
      ],
      "metadata": {
        "id": "BJSUw4ciiHJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nejlep≈°√≠ parametry pro U-Net\n",
        "|max_dist|threshold|sigma|HOTA|tracker|\n",
        "|----------------|------------|-------------|--------------|--|\n",
        "|6| 0.015|0.5|0.69|NN|\n",
        "|6| 0.04|0.5|0.59|Kalman|\n",
        "|8| 0.04|0.5|0.48|Norfair|"
      ],
      "metadata": {
        "id": "4WbYiaVIiK1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vyhodnocen√≠ U-Net\n",
        "\n",
        "detections_per_frame_unet, input = run_detector(model_unet, device, threshold = 0.04, sigma = 0.5)"
      ],
      "metadata": {
        "id": "cn8sFpMoiM7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title V√Ωsledky U-Net pro Nearest-Neighbor tracker\n",
        "\n",
        "val_tracks_nn = link_detections(detections_per_frame_unet, max_dist = 6)\n",
        "results_unet1 = calculate_performance('val_data/val.csv', val_tracks_nn, name=\"U-net Method\")"
      ],
      "metadata": {
        "id": "R-r3BvNsiQXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title V√Ωsledky U-Net pro tracker s kalmanov√Ωm filtrem\n",
        "\n",
        "val_tracks_kalman = track_ccp_sequence(detections_per_frame_unet, max_dist = 6)\n",
        "results_unet2 = calculate_performance('val_data/val.csv', val_tracks_kalman, name=\"U-net Method\")"
      ],
      "metadata": {
        "id": "DctjW9hXiQgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title V√Ωsledky U-Net pro Norfair tracker\n",
        "\n",
        "val_tracks_norf = track_with_norfair(detections_per_frame_unet, max_dist = 8)\n",
        "results_unet3 = calculate_performance('val_data/val.csv', val_tracks_norf, name=\"U-net Method\")"
      ],
      "metadata": {
        "id": "_KHrU13iidC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z uveden√Ωch v√Ωsledk≈Ø lze vidƒõt, ≈æe knihovna Norfair v kombinaci s U-Net nen√≠ efektivn√≠"
      ],
      "metadata": {
        "id": "CTAbB67Pil5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Porovn√°n√≠ dvou tracking≈Ø\n",
        "\n",
        "input_stack = open_tiff_file(\"val_data/val.tif\").astype(np.float64)\n",
        "html = show_tracking_compare(\n",
        "    val_tracks_kalman,\n",
        "    val_tracks_nn,\n",
        "    input_stack,\n",
        "    label_left=\"Kalman\",\n",
        "    label_right=\"NN\",\n",
        "    color_left=\"cyan\",\n",
        "    color_right=\"cyan\"\n",
        ")\n",
        "\n",
        "display(html)"
      ],
      "metadata": {
        "id": "ESxd8RVPij6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vyhodnocen√≠ Res-U-Net\n",
        "Parametry p≈ôi tr√©nov√°n√≠ modelu Res-U-net:\n",
        "- Velikost datasetu 4000\n",
        "- Batch size = 32\n",
        "- Poƒçet epoch = 20\n",
        "- Learning rate = 0.001\n"
      ],
      "metadata": {
        "id": "kwu6bDp5ivox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ladƒõn√≠ parametr≈Ø Res-U-Net\n",
        "\n",
        "df_resunet = run_grid_search(model_resunet, track_method = \"nn\")\n",
        "for md in [ 6, 8]:\n",
        "    plot_heatmap(df_resunet, metric=\"HOTA\", max_dist=md)"
      ],
      "metadata": {
        "id": "QGXWwVJViwfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nejlep≈°√≠ parametry pro Res-U-Net\n",
        "| max_dist|threshold|sigma|HOTA|tracker|\n",
        "|----------------|------------|-------------|--------------|--|\n",
        "|6|0.015|0.5|0.72|NN|\n",
        "|6|0.05|0.5|0.72|NN|\n",
        "|8|0.015|0.5|0.71|NN|"
      ],
      "metadata": {
        "id": "cvQlRVR4i0P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vyhodnocen√≠ Res-U-Net\n",
        "\n",
        "detections_per_frame_resunet, input = run_detector(model_resunet, device, threshold = 0.05, sigma = 1)"
      ],
      "metadata": {
        "id": "nKsq4oO5i2QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vyhodnocen√≠ Res-U-Net pro NN tracker\n",
        "\n",
        "val_tracks_nn_res = link_detections(detections_per_frame_resunet, max_dist = 6)\n",
        "results_resunet1 = calculate_performance('val_data/val.csv', val_tracks_nn_res, name=\"Res-U-net Method\")"
      ],
      "metadata": {
        "id": "o1UOlguZi4S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vyhodnocen√≠ Res-U-Net pro tracker s kalmanov√Ωm filtrem\n",
        "\n",
        "val_tracks_kalman_res = track_ccp_sequence(detections_per_frame_resunet, max_dist = 6)\n",
        "results_resunet2 = calculate_performance('val_data/val.csv', val_tracks_kalman_res, name=\"Res-U-net Method\")"
      ],
      "metadata": {
        "id": "rS7FvPkGjFWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vyhodnocen√≠ Res-U-Net pro Norfair tracker\n",
        "\n",
        "val_tracks_norf_res = track_with_norfair(detections_per_frame_resunet, max_dist = 5)\n",
        "results_resunet3 = calculate_performance('val_data/val.csv', val_tracks_norf_res, name=\"Res-U-net Method\")"
      ],
      "metadata": {
        "id": "G7dOnIkUjHss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Porovn√°n√≠ dvou tracking≈Ø\n",
        "\n",
        "input_stack = open_tiff_file(\"val_data/val.tif\").astype(np.float64)\n",
        "html = show_tracking_compare(\n",
        "    val_tracks_kalman_res,\n",
        "    val_tracks_nn_res,\n",
        "    input_stack,\n",
        "    label_left=\"Kalman\",\n",
        "    label_right=\"NN\",\n",
        "    color_left=\"cyan\",\n",
        "    color_right=\"cyan\"\n",
        ")\n",
        "\n",
        "display(html)"
      ],
      "metadata": {
        "id": "T74Z0DcDjH4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Porovn√°n√≠ dvou model≈Ø na syntetick√©m datasetu\n",
        "\n",
        "img, gt_mask = next(iter(synthetic_dataset))\n",
        "visualize_single_sample_comparison(\n",
        "    model_unet, img,gt_mask,\n",
        "    device, threshold = 0.1, sigma = 0.5,\n",
        "    model_name=\"U-Net\")\n",
        "\n",
        "visualize_single_sample_comparison(\n",
        "    model_resunet,img,gt_mask,\n",
        "    device, threshold = 0.1, sigma = 0.5,\n",
        "    model_name=\"Res-U-Net\")"
      ],
      "metadata": {
        "id": "vX6e5ZcMjICk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vizualizace detekce na obou modelech\n",
        "\n",
        "input_stack = open_tiff_file(\"val_data/val.tif\").astype(np.float64)\n",
        "html = show_detection_compare(\n",
        "    detections_left=detections_per_frame_unet,\n",
        "    detections_right=detections_per_frame_resunet,\n",
        "    image_stack=input_stack,\n",
        "    label_left=\"U-Net\",\n",
        "    label_right=\"Res-U-Net\",\n",
        "    color_left=\"cyan\",\n",
        "    color_right=\"cyan\")\n",
        "\n",
        "display(html)"
      ],
      "metadata": {
        "id": "WYDn_GZ3jM3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SOTA\n",
        "results_sota = calculate_performance('val_data/val.csv', 'val_data/sota.csv', name=\"SOTA Method\")"
      ],
      "metadata": {
        "id": "ChJ1BvZ_jRMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tabulka v√Ωsledk≈Ø\n",
        "\n",
        "summary_data = {\n",
        "    \"Method\": [\"U-Net\", \"Res-U-Net\", \"SOTA Method\"],\n",
        "    \"HOTA\": [results_unet1[\"HOTA\"], results_resunet1[\"HOTA\"], results_sota[\"HOTA\"]],\n",
        "    \"AssA\": [results_unet1[\"AssA\"], results_resunet1[\"AssA\"], results_sota[\"AssA\"]],\n",
        "    \"DetA\": [results_unet1[\"DetA\"], results_resunet1[\"DetA\"], results_sota[\"DetA\"]],\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Round to two decimals for readability\n",
        "summary_df = summary_df.round(2)\n",
        "display(summary_df.style.set_properties(**{\n",
        "    'text-align': 'center',\n",
        "    'font-weight': 'bold'\n",
        "}).set_table_styles([{\n",
        "    'selector': 'th',\n",
        "    'props': [('text-align', 'center'), ('font-weight', 'bold')]\n",
        "}]))"
      ],
      "metadata": {
        "id": "omorNTuyjRYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Porovn√°n√≠ v√Ωsledk≈Ø metod\n",
        "\n",
        "methods = [\"Morphological\", \"CNN baseline\", \"U-Net\", \"ResUNet\", \"SOTA\"]\n",
        "HOTA  = [0.23, 0.31, results_unet1[\"HOTA\"], results_resunet1[\"HOTA\"], results_sota[\"HOTA\"]]\n",
        "AssA  = [0.23, 0.27, results_unet1[\"AssA\"], results_resunet1[\"AssA\"], results_sota[\"AssA\"]]\n",
        "DetA  = [0.24, 0.36, results_unet1[\"DetA\"], results_resunet1[\"DetA\"], results_sota[\"DetA\"]]\n",
        "\n",
        "x = np.arange(len(methods))\n",
        "width = 0.25\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(x - width, HOTA,  width, label=\"HOTA\")\n",
        "plt.bar(x,          AssA, width, label=\"AssA\")\n",
        "plt.bar(x + width,  DetA, width, label=\"DetA\")\n",
        "\n",
        "plt.xticks(x, methods, rotation=30)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Comparison of tracking performance across methods\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y9Bnj7qejWKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title V√Ωpoƒçet precission a recall\n",
        "\n",
        "def match_points(gt_points, pred_points, max_dist=5):\n",
        "    if len(gt_points) == 0 and len(pred_points) == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    if len(gt_points) == 0:\n",
        "        return 0, len(pred_points), 0\n",
        "\n",
        "    if len(pred_points) == 0:\n",
        "        return 0, 0, len(gt_points)\n",
        "\n",
        "    dists = spatial.distance.cdist(gt_points, pred_points)\n",
        "\n",
        "    # Hungarian matching\n",
        "    gt_idx, pred_idx = linear_sum_assignment(dists)\n",
        "\n",
        "    # keep only matches within max_dist\n",
        "    valid = dists[gt_idx, pred_idx] <= max_dist\n",
        "\n",
        "    tp = np.sum(valid)\n",
        "    fp = len(pred_points) - tp\n",
        "    fn = len(gt_points) - tp\n",
        "\n",
        "    return tp, fp, fn\n",
        "\n",
        "def compute_precision_recall(gt_df, detections_per_frame, max_dist=5):\n",
        "    TP = FP = FN = 0\n",
        "\n",
        "    for frame in range(len(detections_per_frame)):\n",
        "        gt_f = gt_df[gt_df.frame == frame]\n",
        "        gt_points = gt_f[[\"x\", \"y\"]].values\n",
        "        pred_points = np.array(detections_per_frame[frame])\n",
        "\n",
        "        tp, fp, fn = match_points(gt_points, pred_points, max_dist)\n",
        "\n",
        "        TP += tp\n",
        "        FP += fp\n",
        "        FN += fn\n",
        "\n",
        "    precision = TP / (TP + FP + 1e-9)\n",
        "    recall = TP / (TP + FN + 1e-9)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1\": f1,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"FN\": FN\n",
        "    }\n",
        "\n",
        "def tracks_to_detections_per_frame(tracks_df, num_frames):\n",
        "    detections = [[] for _ in range(num_frames)]\n",
        "\n",
        "    for _, row in tracks_df.iterrows():\n",
        "        f = int(row.frame)\n",
        "        detections[f].append((row.x, row.y))\n",
        "\n",
        "    return detections\n",
        "\n",
        "gt = pd.read_csv(\"val_data/val.csv\")\n",
        "sota_tracks = pd.read_csv(\"val_data/sota.csv\")\n",
        "\n",
        "detections_sota = tracks_to_detections_per_frame(\n",
        "    sota_tracks,\n",
        "    num_frames=len(input)\n",
        ")\n"
      ],
      "metadata": {
        "id": "_wqwrIWRjZ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Precission a recall porovn√°n√≠\n",
        "\n",
        "gt = pd.read_csv(\"val_data/val.csv\")\n",
        "\n",
        "pr_unet = compute_precision_recall(gt, detections_per_frame_unet, max_dist=6)\n",
        "pr_resunet = compute_precision_recall(gt, detections_per_frame_resunet, max_dist=6)\n",
        "pr_sota = compute_precision_recall(gt, detections_sota, max_dist=6)\n",
        "\n",
        "pd.DataFrame([pr_sota, pr_unet, pr_resunet],\n",
        "             index=[\"SOTA\", \"U-Net\", \"ResUNet\"])"
      ],
      "metadata": {
        "id": "6_Sh8sRPjcXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Nejlep≈°√≠ model\n",
        "\n",
        "model_best = model_resunet\n",
        "detections_per_frame_best, input = run_detector(model_best, device, threshold = 0.05, sigma = 0.5)\n",
        "val_tracks = link_detections(detections_per_frame_best, max_dist = 6)\n",
        "best_results = calculate_performance('val_data/val.csv', val_tracks, name=\"Best Method\")\n",
        "\n",
        "input_stack = open_tiff_file(\"val_data/val.tif\").astype(np.float64)\n",
        "show_tracking(val_tracks, input_stack )"
      ],
      "metadata": {
        "id": "DslzswcQjgTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Z√°vƒõr\n",
        "Byla analyzov√°na data bunƒõk CCP pomoc√≠ nƒõkolika metod strojov√©ho uƒçen√≠ s c√≠lem p≈ôibl√≠≈æit se v√Ωsledk≈Øm referenƒçn√≠ metody SOTA nebo je p≈ôekonat. Za nej√∫spƒõ≈°nƒõj≈°√≠ metodu je dle v√Ωsledk≈Ø pova≈æov√°na metoda ResUNet spolu s NN trackingem, kter√° se li≈°√≠ od SOTA v√Ωsledk≈Ø v r√°mci HOTA pouze o 0.02. P≈ôesto≈æe nebyla p≈ôekon√°na referenƒçn√≠ metoda SOTA, byly nalezeny metody a parametry, kter√© se k jej√≠m hodnot√°m v r√°mci metrik HOTA, AssA a DetA velice bl√≠≈æ√≠\n",
        "\n",
        "Pro lep≈°√≠ v√Ωsledky by se nab√≠zely n√°sleduj√≠c√≠ smƒõry:\n",
        "Pou≈æit√≠ pokroƒçilej≈°√≠ch model≈Ø (nap≈ô. UNet++ nebo men≈°√≠ ViT-based architektury).\n",
        "Zkusit jinou loss funkci, nap≈ô√≠klad kombinace BCE + Dice.\n",
        "\n",
        "Pokroƒçilej≈°√≠ tracking, nap≈ô. DeepSORT, multi-hypothesis tracking nebo probabilistic data association.\n"
      ],
      "metadata": {
        "id": "RyWMzSHWjjfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reference\n",
        " Prince, S. J. D. (2023). *Understanding Deep Learning*. MIT Press\n",
        "\n",
        " (https://github.com/tryolabs/norfair)\n",
        "\n"
      ],
      "metadata": {
        "id": "6LjHPrzljkhI"
      }
    }
  ]
}