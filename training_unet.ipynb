{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNvCjdRr6S4T6m2vsBqO1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/su2-final-projects-2025-nesnerova/blob/main/training_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/Strojove-uceni/su2-final-projects-2025-nesnerova.git project\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/project\")\n"
      ],
      "metadata": {
        "id": "3AqQ5oRzRLVj",
        "outputId": "86e7914f-a9b4-406f-ab2e-8ffe5fab0f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 26 (delta 6), reused 1 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (26/26), 64.37 KiB | 5.85 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import knihoven\n",
        "\n",
        "import os, requests, zipfile\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import subprocess\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import ScalarMappable\n",
        "from matplotlib.colors import Normalize\n",
        "import matplotlib.collections as mc\n",
        "import pandas as pd\n",
        "from matplotlib.patches import Rectangle\n",
        "from torch.utils.data import IterableDataset\n",
        "from typing import Tuple, Union\n",
        "from scipy.ndimage import gaussian_filter, maximum_filter\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as torch_data\n",
        "import time\n",
        "import threading\n",
        "import random\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hUu7DD_IEH5-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "bN_M7iP6BmtU",
        "outputId": "5ec8150d-3e67-426a-802b-496d8bf55b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîê Downloading SSL certificate chain...\n",
            "‚úÖ Certificate chain downloaded.\n",
            "\n",
            "üì• Downloading val_and_sota.zip (approx 1min)...\n",
            "‚úÖ val_and_sota.zip downloaded successfully (452414908 bytes)\n",
            "üìÇ Extracting to '/content/val_data/'...\n",
            "‚úÖ Extraction completed to '/content/val_data'\n",
            "\n",
            "üéØ All done!\n"
          ]
        }
      ],
      "source": [
        "#@title Sta≈æen√≠ dat\n",
        "\n",
        "def download_and_unzip(url, extract_to, chain_path):\n",
        "    \"\"\"Download and unzip using a verified SSL certificate.\"\"\"\n",
        "    if os.path.exists(extract_to):\n",
        "        print(f\"The directory '{extract_to}' already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    local_zip = os.path.basename(url)\n",
        "    print(f\"üì• Downloading {local_zip} (approx 1min)...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, verify=chain_path, timeout=20)\n",
        "        response.raise_for_status()\n",
        "        with open(local_zip, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"‚úÖ {local_zip} downloaded successfully ({os.path.getsize(local_zip)} bytes)\")\n",
        "\n",
        "        print(f\"üìÇ Extracting to '{extract_to}/'...\")\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        with zipfile.ZipFile(local_zip, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        os.remove(local_zip)\n",
        "        print(f\"‚úÖ Extraction completed to '{extract_to}'\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Download error: {e}\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"‚ùå The downloaded file is not a valid ZIP archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Unexpected error: {e}\")\n",
        "\n",
        "chain_path = \"/content/chain-harica-cross.pem\"\n",
        "print(\"üîê Downloading SSL certificate chain...\")\n",
        "cert_url = \"https://pki.cesnet.cz/_media/certs/chain-harica-rsa-ov-crosssigned-root.pem\"\n",
        "r = requests.get(cert_url, timeout=10, stream=True)\n",
        "r.raise_for_status()\n",
        "with open(chain_path, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "print(\"‚úÖ Certificate chain downloaded.\\n\")\n",
        "zip_url = \"https://su2.utia.cas.cz/files/labs/final2025/val_and_sota.zip\"\n",
        "extract_directory = \"/content/val_data\"\n",
        "download_and_unzip(zip_url, extract_directory, chain_path)\n",
        "\n",
        "print(\"\\nüéØ All done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Z√°kladn√≠ funkce\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Ensure deterministic behavior in cuDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "class OTF:\n",
        "    \"\"\"\n",
        "    The Optical Transfer Function of the optical system.\n",
        "    Generates a 2-D image from an exponential approximation of the ideal OTF.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, na: float, wavelength: float, pixel_size: float, image_size: int, curvature: float):\n",
        "        \"\"\"\n",
        "        :param na: Numerical aperture of the optical system.\n",
        "        :param wavelength: Wavelength of the emitted light in nanometers.\n",
        "        :param pixel_size: Physical size of the image pixel in micrometers.\n",
        "        :param image_size: Width and height of the image.\n",
        "        :param curvature: Bend of the model OTF function in [0, 1] range where 1 is a perfect OTF.\n",
        "        :param samples: Number of values to sample.\n",
        "        \"\"\"\n",
        "        cutoff_frequency = 1000 * 2 * na / wavelength  # in micrometer^-1 (for incoherent imaging)\n",
        "        self.image_cutoff = cutoff_frequency * pixel_size * image_size\n",
        "        self.image_size = image_size\n",
        "        self.curvature = curvature\n",
        "\n",
        "    def __call__(self, size: int = None, x_shift: float = 0, y_shift: float = 0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate a 2-D image representation.\n",
        "        :param size: Width and height of the generated image.\n",
        "        :param x_shift: Sub-pixel shift along the x-axis of the OTF center.\n",
        "        :param y_shift:Sub-pixel shift along the y-axis of the OTF center.\n",
        "        :return: 2-D representation of the OTF\n",
        "        \"\"\"\n",
        "        if size is None:\n",
        "            size = self.image_size\n",
        "\n",
        "        x, y = np.meshgrid(np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]),\n",
        "                           np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]))\n",
        "        distance_to_origin = np.hypot(x + x_shift, y + y_shift)\n",
        "\n",
        "        return self.value(np.minimum(distance_to_origin / self.image_cutoff, 1))\n",
        "\n",
        "    def double(self, x_shift: float = 0, y_shift: float = 0) -> np.ndarray:\n",
        "        return self(self.image_size * 2, x_shift, y_shift)\n",
        "\n",
        "    def value(self, x):\n",
        "        return (2 / np.pi) * (np.arccos(x) - x * np.sqrt(1 - x * x)) * self.curvature ** x\n",
        "\n",
        "def apodization_filter(dist_ratio: float, bend: float, size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate an apodization filter that can be directly multiplied with the summed fft result image.\n",
        "\n",
        "    :param dist_ratio: A coefficient that transform the point distance in pixels to a value where [0, 1] range\n",
        "        corresponds to the extended OTF support.\n",
        "    :param bend: A coefficient in the [0, 1] range defining how much medium frequencies should be augmented.\n",
        "    :param size: Size of the apodization filter.\n",
        "    :return: np.ndarray of size (size, size) of the apodization filter.\n",
        "    \"\"\"\n",
        "    x, y = np.meshgrid(np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]),\n",
        "                       np.hstack([np.arange(size // 2), np.arange(-size // 2, 0)]))\n",
        "\n",
        "    distance = np.hypot(x, y) * dist_ratio\n",
        "    mask = np.bitwise_and(0 <= distance, distance < 1)\n",
        "\n",
        "    apo = np.power(\n",
        "        (2 / np.pi) * (np.arccos(distance, where=mask) - distance * np.sqrt(1 - distance * distance, where=mask)),\n",
        "        bend,\n",
        "        where=mask\n",
        "    )\n",
        "\n",
        "    return np.where(mask, apo, 0)\n",
        "\n",
        "\n",
        "def wiener_filter(shifts, otf: OTF, w: float, size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate a wiener filter denominator so that it can be directly multiplied with the summed fft result image.\n",
        "\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :param otf: OTF object.\n",
        "    :param w: Wiener parameter.\n",
        "    :param size: Size of the wiener filter.\n",
        "    :return: np.ndarray of size (size, size) of the Wiener filter denominator.\n",
        "    \"\"\"\n",
        "    otf0 = np.abs(otf(size)) ** 2\n",
        "    wiener = 3 * otf0\n",
        "\n",
        "    for i in range(3):\n",
        "        otf1 = np.abs(otf(size, shifts[i][1], shifts[i][0])) ** 2\n",
        "        otf2 = np.abs(otf(size, -shifts[i][1], -shifts[i][0])) ** 2\n",
        "\n",
        "        wiener += otf1 + otf2\n",
        "\n",
        "    return 1 / (wiener + w * w)\n",
        "\n",
        "def map_otf_support(components: np.ndarray, shifts, otf: OTF) -> None:\n",
        "    \"\"\"\n",
        "    Multiply conjugated OTF to corresponding parts of it's support in the separated and shifted components. Values\n",
        "    outside the OTF support are set to zero. This function alters the original images.\n",
        "\n",
        "    :param components: np.ndarray of shape (9, size, size) of padded separated and shifted components in frequency space.\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :param otf: OTF object\n",
        "    \"\"\"\n",
        "    size = components.shape[-1]\n",
        "\n",
        "    components[::3] *= np.conjugate(otf(size))\n",
        "    for i in range(3):\n",
        "        components[i * 3 + 1] *= np.conjugate(otf(size, shifts[i][1], shifts[i][0]))\n",
        "        components[i * 3 + 2] *= np.conjugate(otf(size, -shifts[i][1], -shifts[i][0]))\n",
        "\n",
        "\n",
        "def fourier_shift(fft_image: np.ndarray, x_shift: Union[float, np.ndarray], y_shift: Union[float, np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Use the fourier shift theorem to perform a sub-pixel shift of a frequency image in the spatial domain.\n",
        "\n",
        "    It is implemented in a way that allows the x_shift and y_shift parameters to be a np.ndarray of arbitrary shape with\n",
        "    the last two axes being np.newaxis or None. The returned array shape is then x/y_shift.shape + (height, width)\n",
        "\n",
        "    :param fft_image: np.ndarray of shape (height, width).\n",
        "    :param x_shift: Sub-pixel shift along the x-axis.\n",
        "    :param y_shift: Sub-pixel shift along the y-axis.\n",
        "    :return: np.ndarray of shape (height, width) of the shifted frequency image.\n",
        "    \"\"\"\n",
        "    height, width = fft_image.shape[-2:]\n",
        "\n",
        "    spatial_image = np.fft.ifft2(fft_image)\n",
        "\n",
        "    x_indices = np.arange(-width // 2, width // 2, dtype=int)\n",
        "    y_indices = np.arange(-height // 2, height // 2, dtype=int)\n",
        "\n",
        "    x = np.exp(-1j * 2 * np.pi * x_shift * x_indices[None, :] / width)\n",
        "    y = np.exp(-1j * 2 * np.pi * y_shift * y_indices[:, None] / height)\n",
        "\n",
        "    shifted_spatial_image = spatial_image * (x * y)\n",
        "\n",
        "    return np.fft.fft2(shifted_spatial_image)\n",
        "\n",
        "\n",
        "def shift_components(components: np.ndarray, shifts) -> None:\n",
        "    \"\"\"\n",
        "    Use the fourier shift theorem to perform a sub-pixel shift of frequency components in the spatial domain. This\n",
        "    function alters the original images.\n",
        "\n",
        "    :param components: np.ndarray of shape (9, height, width) of padded separated components in frequency space.\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :return: np.ndarray of shape (9, height, width) of shifted components.\n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        components[i * 3 + 1, ...] = fourier_shift(components[i * 3 + 1, ...], shifts[i][1], shifts[i][0])\n",
        "        components[i * 3 + 2, ...] = fourier_shift(components[i * 3 + 2, ...], -shifts[i][1], -shifts[i][0])\n",
        "\n",
        "\n",
        "def pad_components(components: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pad an image to twice its size from the center, keeping the corners intact.\n",
        "    This makes it possible to pad images that are NOT centered using np.fft.fftshift.\n",
        "\n",
        "    :param components: np.ndarray of shape (..., size, size) of separated components in frequency space.\n",
        "    :return: np.ndarray of shape (..., size, size) padded with zeros from the center.\n",
        "    \"\"\"\n",
        "    size = components.shape[-1]\n",
        "    padded_components = np.zeros(components.shape[:-2] + (size * 2, size * 2), dtype=np.complex128)\n",
        "\n",
        "    x, y = np.meshgrid(\n",
        "        np.hstack([np.arange(size // 2), np.arange(size * 3 // 2, size * 2)]),\n",
        "        np.hstack([np.arange(size // 2), np.arange(size * 3 // 2, size * 2)])\n",
        "    )\n",
        "\n",
        "    padded_components[..., y, x] = components\n",
        "\n",
        "    return padded_components\n",
        "\n",
        "def _component_separation_matrix(phase_offset: float = 0, modulation: float = 1) -> np.ndarray:\n",
        "    phases = np.array([0, 2 * np.pi / 3, 4 * np.pi / 3]) + phase_offset\n",
        "\n",
        "    M = np.ones((3, 3), dtype=np.complex128)\n",
        "\n",
        "\n",
        "    M[:, 1] = 0.5 * modulation * (np.cos(phases) + 1j * np.sin(phases))\n",
        "    M[:, 2] = 0.5 * modulation * (np.cos(-phases) + 1j * np.sin(-phases))\n",
        "\n",
        "    return np.linalg.inv(M)\n",
        "\n",
        "def separate_components(fft_images: np.ndarray, phase_offsets=np.zeros(3), phase_modulations=np.ones(3)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the spectral separation of components.\n",
        "\n",
        "    :param fft_images: np.ndarray of shape (9, height, width) of images in frequency space.\n",
        "    :param phase_offsets: A container of length 3 of phase offsets for each of the 3 orientations, zeros by default.\n",
        "    :param phase_modulations: A container of length 3 of phase modulations for each of the 3 orientations,\n",
        "        ones by default.\n",
        "    :return: np.ndarray of shape (9, height, width) of separated components.\n",
        "    \"\"\"\n",
        "    separation_matrices = np.array([_component_separation_matrix(phase_offsets[i], phase_modulations[i])\n",
        "                                    for i in range(3)])\n",
        "\n",
        "    return np.einsum('aij,ajkl->aikl',separation_matrices,\n",
        "                     fft_images.reshape(3, 3, *fft_images.shape[1:])).reshape(9, *fft_images.shape[1:])\n",
        "\n",
        "\n",
        "def run_reconstruction(fft_images: np.ndarray, otf: OTF, shifts, phase_offsets, modulations, config: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Perform a SIM reconstruction using the provided parameters.\n",
        "\n",
        "    :param fft_images: np.ndarray of shape (9, height, width) of images in frequency space.\n",
        "    :param otf: OTF object\n",
        "    :param shifts: A container of length 3 of shifts of the second component for each of the 3 orientations.\n",
        "    :param phase_offsets: A container of length 3 of phase offsets for each of the 3 orientations.\n",
        "    :param modulations: A container of length 3 of phase modulations for each of the 3 orientations.\n",
        "    :param config: A dictionary of SIM reconstruction parameters. It must contain the following keys: [\"wavelength\",\n",
        "        \"na\", \"px_size\", \"apo_cutoff\", \"wiener_parameter\", \"apo_bend\"]\n",
        "    :return: fft_result, spatial_result\n",
        "    \"\"\"\n",
        "    size = fft_images.shape[-1]\n",
        "    cutoff = 1000 * 2 * config[\"na\"] / config[\"wavelength\"]\n",
        "    apo_dist_ratio = 1 / (config[\"px_size\"] * config[\"apo_cutoff\"] * cutoff * size)\n",
        "\n",
        "    components = separate_components(fft_images, phase_offsets, modulations)\n",
        "    components = pad_components(components)\n",
        "\n",
        "    shift_components(components, shifts)\n",
        "    map_otf_support(components, shifts, otf)\n",
        "\n",
        "    wiener = wiener_filter(shifts, otf, config[\"wiener_parameter\"], size * 2)\n",
        "    apodization = apodization_filter(apo_dist_ratio, config[\"apo_bend\"], size * 2)\n",
        "\n",
        "    fft_result = np.sum(components, 0) * wiener * apodization\n",
        "    spatial_result = np.real(np.fft.ifft2(fft_result))\n",
        "\n",
        "    return fft_result, spatial_result\n",
        "\n",
        "def illumination_pattern(angle, frequency, phase_offset, amplitude, size) -> np.ndarray:\n",
        "    n = size // 2\n",
        "    Y, X = np.mgrid[-n:n, -n:n]\n",
        "    ky, kx = np.sin(angle) * frequency, np.cos(angle) * frequency\n",
        "    return 1 + amplitude * np.cos(2 * np.pi * (X * kx + Y * ky) + phase_offset)\n",
        "\n",
        "\n",
        "class PerlinNoise:\n",
        "    \"\"\"Adapted from: https://github.com/pvigier/perlin-numpy\"\"\"\n",
        "\n",
        "    def __init__(self, size: int, res: int):\n",
        "        \"\"\"\n",
        "        :param size: length of both dimensions of the generated noise image\n",
        "        :param res: number of periods of noise to generate along each axis\n",
        "        \"\"\"\n",
        "\n",
        "        meshgrid = np.mgrid[0:res:res / size, 0:res:res / size]\n",
        "        self.grid = np.stack(meshgrid) % 1\n",
        "\n",
        "        self.t = self._fade(self.grid)\n",
        "        self.d = size // res\n",
        "        self.sample_size = (res + 1, res + 1)\n",
        "\n",
        "    def __call__(self) -> np.array:\n",
        "        \"\"\"\n",
        "        Returned values are always in the [0, 1] range with ~0.5 mean\n",
        "        \"\"\"\n",
        "        angles = 2 * np.pi * np.random.random_sample(self.sample_size)\n",
        "        gradients = np.dstack((np.cos(angles), np.sin(angles))).repeat(self.d, 0).repeat(self.d, 1)\n",
        "\n",
        "        n00 = (np.dstack((self.grid[0], self.grid[1])) * gradients[:-self.d, :-self.d]).sum(2)\n",
        "        n10 = (np.dstack((self.grid[0] - 1, self.grid[1])) * gradients[self.d:, :-self.d]).sum(2)\n",
        "        n01 = (np.dstack((self.grid[0], self.grid[1] - 1)) * gradients[:-self.d, self.d:]).sum(2)\n",
        "        n11 = (np.dstack((self.grid[0] - 1, self.grid[1] - 1)) * gradients[self.d:, self.d:]).sum(2)\n",
        "\n",
        "        n0 = n00 * (1 - self.t[0]) + n10 * self.t[0]\n",
        "        n1 = n01 * (1 - self.t[0]) + n11 * self.t[0]\n",
        "\n",
        "        return 0.5 + 2 ** -0.5 * (n0 * (1 - self.t[1]) + n1 * self.t[1])\n",
        "\n",
        "    @staticmethod\n",
        "    def _fade(t):\n",
        "        return 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3\n",
        "\n",
        "\n",
        "class SyntheticDataset(IterableDataset):\n",
        "    def __init__(self, contrast_fg_range: tuple[float,float] = (0.0, 1.0), contrast_bg_range: tuple[float,float] = (0.0, 1.0)):\n",
        "        self.patch_size = 128\n",
        "\n",
        "        self.contrast_fg_range = contrast_fg_range\n",
        "        self.contrast_bg_range = contrast_bg_range\n",
        "\n",
        "        self.frequency = 0.17\n",
        "        self.amplitude = 1.0\n",
        "\n",
        "        self.config = {\n",
        "            \"na\": 1.49,\n",
        "            \"wavelength\": 512,\n",
        "            \"px_size\": 0.07,\n",
        "            \"wiener_parameter\": 0.1,\n",
        "            \"apo_cutoff\": 2.0,\n",
        "            \"apo_bend\": 0.9\n",
        "        }\n",
        "\n",
        "        self.otf = OTF(self.config['na'], self.config['wavelength'], self.config['px_size'], self.patch_size // 2, 0.3)\n",
        "        self.otf_mult = self.otf(self.patch_size)\n",
        "\n",
        "        self.perlin = PerlinNoise(self.patch_size, 1)\n",
        "\n",
        "    def _simulate_sim(self, image):\n",
        "        angle0 = np.random.uniform(0, np.pi * 2)\n",
        "        phase_offsets = np.random.uniform(0, np.pi * 2, 3)\n",
        "\n",
        "        shifts = [(self.frequency * self.patch_size * np.sin(angle0 + i * np.pi / 3),\n",
        "                   self.frequency * self.patch_size * np.cos(angle0 + i * np.pi / 3))\n",
        "                  for i in range(3)]\n",
        "\n",
        "        illumination = np.stack([illumination_pattern(angle0 + i // 3 * np.pi / 3,\n",
        "                                                      self.frequency,\n",
        "                                                      phase_offsets[i // 3] + (i % 3) * np.pi * 2 / 3,\n",
        "                                                      self.amplitude,\n",
        "                                                      self.patch_size)\n",
        "                                 for i in range(9)])\n",
        "\n",
        "\n",
        "        fg_c = np.random.uniform(*self.contrast_fg_range)\n",
        "        bg_c = np.random.uniform(*self.contrast_bg_range)\n",
        "        foreground = 250 + fg_c * 500\n",
        "        background = 50 + bg_c * 50\n",
        "\n",
        "        high_res_image = (image * foreground + background) * self.perlin()\n",
        "\n",
        "        ix = np.fft.fft2(illumination * high_res_image)\n",
        "        hix = self.otf_mult * ix\n",
        "        dhix = hix.reshape(9, 2, self.patch_size // 2, 2, self.patch_size // 2).sum((1, 3)) / 4\n",
        "        low_res_images = np.random.poisson(np.fft.ifft2(dhix).real).astype(np.float64)\n",
        "\n",
        "        noisy_shifts = [np.random.triangular((y - 0.25, x - 0.25), (y, x), (y + 0.25, x + 0.25)) for y, x in shifts]\n",
        "        noisy_phase_offsets = np.random.normal(phase_offsets, np.pi / 6)\n",
        "        noisy_amplitudes = np.random.normal(self.amplitude, 0.1, 3)\n",
        "\n",
        "        reconstruction = run_reconstruction(np.fft.fft2(low_res_images), self.otf, noisy_shifts, noisy_phase_offsets, noisy_amplitudes, self.config)[1]\n",
        "        return (reconstruction - np.mean(reconstruction)) / np.std(reconstruction)\n",
        "\n",
        "class SyntheticCCPDataset(SyntheticDataset):\n",
        "    def __init__(self,min_n: int = 5, max_n: int = 15, radius: float = 2.5,\n",
        "        contrast_fg_range: tuple[float,float] = (0.0, 1.0),\n",
        "        contrast_bg_range: tuple[float,float] = (0.0, 1.0)):\n",
        "        super().__init__(\n",
        "            contrast_fg_range=contrast_fg_range,\n",
        "            contrast_bg_range=contrast_bg_range\n",
        "        )\n",
        "\n",
        "        self.patch_size = 128\n",
        "\n",
        "        # Possible positions\n",
        "        yy, xx = np.mgrid[15:self.patch_size - 1:16, 15:self.patch_size - 1:16]\n",
        "        self.yy = yy.flatten()\n",
        "        self.xx = xx.flatten()\n",
        "\n",
        "        # validate\n",
        "        if not (0 <= min_n < max_n <= 49):\n",
        "            raise ValueError(\"Require 0 <= min_n < max_n <= 49\")\n",
        "        self.min_n, self.max_n = min_n, max_n\n",
        "\n",
        "        self.max_offset = 8\n",
        "        assert self.max_offset < 16\n",
        "\n",
        "        # Beta params\n",
        "        self.beta_a = 2\n",
        "        self.beta_b = 1\n",
        "\n",
        "        # CCP shape params\n",
        "        self.radius = radius\n",
        "        self.thickness = 1.0\n",
        "\n",
        "        # Patch positions\n",
        "        self.yyy, self.xxx = np.mgrid[:self.patch_size, :self.patch_size]\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            yield self.data_sample()\n",
        "\n",
        "    def data_sample(self):\n",
        "        # Generate positions and classes\n",
        "        n = np.random.randint(self.min_n, self.max_n)\n",
        "        indices = np.random.choice(len(self.yy), size=n, replace=False)\n",
        "        offsets = np.random.uniform(-self.max_offset, self.max_offset, (n, 2))\n",
        "        positions = np.column_stack([self.yy[indices], self.xx[indices]]) + offsets\n",
        "        classes = np.random.beta(self.beta_a, self.beta_b, n) * 0.9 + 0.1\n",
        "\n",
        "        # Generate simulated HR image and output target\n",
        "        target_distance = classes * self.radius\n",
        "        distance = np.hypot(self.yyy[..., None] - positions[:, 0], self.xxx[..., None] - positions[:, 1])\n",
        "        abs_distance = np.abs(distance - target_distance)\n",
        "        parts = np.where(abs_distance > self.thickness, 0, np.log(np.interp(abs_distance / self.thickness, [0, 1], [np.e, 1])))\n",
        "        full_image = np.sum(parts, -1)\n",
        "\n",
        "        distances = np.maximum(classes - distance / ((1 - classes) * 2 + self.radius + self.thickness * 2), 0)\n",
        "        y = np.minimum(np.sum(distances, -1), 1)\n",
        "\n",
        "        # Generate simulated SIM image\n",
        "        x = super()._simulate_sim(full_image)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4ZUfMxeUEIpX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pytorch Dataset\n",
        "\n",
        "class SyntheticCCPDatasetTorch(torch_data.Dataset):\n",
        "    \"\"\"PyTorch Dataset that yields synthetic CCP images and masks. \"\"\"\n",
        "    # Changing these parameters too much could result in data that doesn‚Äôt match the real validation or test set.\n",
        "    _min_n = 5\n",
        "    _max_n = 15\n",
        "    _radius = 2.5\n",
        "    _contrast_fg_range = (0.0, 1.0)\n",
        "    _contrast_bg_range = (0.0, 1.0)\n",
        "\n",
        "    def __init__(self, length: int = 500):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        # create one generator with the fixed parameters\n",
        "        self._synthetic = SyntheticCCPDataset(\n",
        "            min_n=self._min_n,\n",
        "            max_n=self._max_n,\n",
        "            radius=self._radius,\n",
        "            contrast_fg_range=self._contrast_fg_range,\n",
        "            contrast_bg_range=self._contrast_bg_range,\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # draw a new synthetic sample\n",
        "        img, mask = self._synthetic.data_sample()\n",
        "\n",
        "        # Normalise image to [0, 1] for better training stability\n",
        "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors and add channel dimension\n",
        "        img_tensor = torch.from_numpy(img).unsqueeze(0).float()\n",
        "        mask_tensor = torch.from_numpy(mask).unsqueeze(0).float()\n",
        "\n",
        "        return img_tensor, mask_tensor"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Sau34VBfDo6b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title U-Net detektor\n",
        "\n",
        "# -------------------------------\n",
        "# üî∂ Double Convolution Block\n",
        "# -------------------------------\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(Conv ‚Üí ReLU) √ó 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# üî∂ Downsampling Block\n",
        "# -------------------------------\n",
        "class Down(nn.Module):\n",
        "    \"\"\"MaxPool ‚Üí DoubleConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# üî∂ Upsampling Block with Cropping Fix\n",
        "# -------------------------------\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upsampling ‚Üí Concatenate ‚Üí DoubleConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # ---------------------------\n",
        "        # üî• FIX ‚Äî crop encoder maps\n",
        "        # aby se shodovaly rozmƒõry\n",
        "        # ---------------------------\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x2 = x2[:, :, diffY // 2 : x2.size()[2] - diffY // 2,\n",
        "                     diffX // 2 : x2.size()[3] - diffX // 2]\n",
        "\n",
        "        # Skip connection\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# üî∂ FULL U-Net MODEL\n",
        "# -------------------------------\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels=1, n_classes=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)  # bottleneck\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "\n",
        "        # Output Layer (heatmap)\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VroXRguPD8aG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tr√©nov√°n√≠ modelu\n",
        "\n",
        "def train_detector(model: nn.Module,\n",
        "                       dataset: torch_data.Dataset,\n",
        "                       device: torch.device,\n",
        "                       batch_size: int = 16,\n",
        "                       num_epochs: int = 3,\n",
        "                       lr: float = 1e-3,):\n",
        "\n",
        "    loader = torch_data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    loss_history = []\n",
        "    start_epoch = 0\n",
        "    # === TRAIN LOOP ===\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * images.size(0)\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataset)\n",
        "        loss_history.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "def train_experiment(length=1000, batch=16, epochs=5, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Starts training with any parameters and with model Res-U-Net or U-net.\n",
        "    \"\"\"\n",
        "    global model, synthetic_dataset, device\n",
        "\n",
        "    print(f\"\\nüöÄ Training with parameters:\")\n",
        "    print(f\"  Dataset length = {length}\")\n",
        "    print(f\"  Batch size     = {batch}\")\n",
        "    print(f\"  Epochs         = {epochs}\")\n",
        "    print(f\"  Learning rate  = {lr}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    synthetic_dataset = SyntheticCCPDatasetTorch(length=length)\n",
        "\n",
        "    model = UNet()\n",
        "    model.to(device)\n",
        "\n",
        "    try:\n",
        "        # Train\n",
        "        loss_history = train_detector(\n",
        "            model,\n",
        "            synthetic_dataset,\n",
        "            device,\n",
        "            batch_size=batch,\n",
        "            num_epochs=epochs,\n",
        "            lr=lr\n",
        "        )\n",
        "\n",
        "        # Plot loss\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.plot(loss_history, marker='o')\n",
        "        plt.title(\"Loss bƒõhem tr√©nov√°n√≠\")\n",
        "        plt.xlabel(\"Epochy\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid(True)\n",
        "        plt.savefig(\"loss_history.pdf\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\n‚úÖ Training finished!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training crashed with error: {e}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "bJiJCCiDD2e8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Spu≈°tƒõn√≠ U-Net\n",
        "\"\"\"\n",
        "Runs U-Net training with specified parameters.\n",
        "\"\"\"\n",
        "DATASET_LENGTH = 4000\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "model_unet = train_experiment(DATASET_LENGTH, BATCH_SIZE, EPOCHS, LEARNING_RATE)"
      ],
      "metadata": {
        "id": "WVHOwRK6D9vu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ulo≈æen√≠\n",
        "from google.colab import files\n",
        "torch.save(model_unet.state_dict(), \"unet_trained.pth\")\n",
        "files.download(\"unet_trained.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "49ZtiE3NGORe",
        "outputId": "9639695c-e579-47e5-ad35-8d81798f63f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f011bd0-eee2-4345-b949-e73d42ddb13a\", \"unet_trained.pth\", 124140957)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}